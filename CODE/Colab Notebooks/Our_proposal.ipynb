{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"iCaRL_OFFICIAL_DynamicK.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"qGeu47FfPs5I","colab_type":"text"},"source":["# **Incremental learning - Project**\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"4CwlqKl4RzLi","colab_type":"text"},"source":["**Install requirements**"]},{"cell_type":"code","metadata":{"id":"k9O3aM3Tb28q","colab_type":"code","colab":{}},"source":["#!pip3 install 'torch==1.3.1'\n","#!pip3 install 'torchvision==0.5.0'\n","#!pip3 install 'Pillow-SIMD'\n","#!pip3 install 'tqdm'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MrkE-2MVbZjU","colab_type":"text"},"source":["**Import models and functions**"]},{"cell_type":"code","metadata":{"id":"uOGd-aASa40O","colab_type":"code","colab":{}},"source":["import os\n","\n","if not os.path.isdir('./models'):\n","  !git clone https://github.com/gabrieletiboni/Incremental-learning-on-image-recognition.git\n","  !cp -r \"/content/Incremental-learning-on-image-recognition/CODE/utils\" \"/content/\"\n","  !cp -r \"/content/Incremental-learning-on-image-recognition/CODE/models\" \"/content/\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"esZgBn2rBV9L","colab_type":"text"},"source":["**Import libraries**"]},{"cell_type":"code","metadata":{"id":"uyDs3TD9Bda1","colab_type":"code","colab":{}},"source":["import sys\n","\n","import torch\n","import torch.nn as nn\n","from torch.backends import cudnn\n","\n","import datetime\n","from pytz import timezone\n","import os\n","import math\n","import time\n","import random\n","\n","\n","from torchvision.datasets import CIFAR100\n","from torchvision.transforms import transforms\n","import torchvision.utils\n","from torch.utils.data import Subset, DataLoader\n","import torch.optim as optim\n","\n","from models.ResNet import resnet32\n","# Below a modified version that best represents the same ResNet32 used by iCaRL\n","# from models.ResNet_iCaRLVersion import resnet32\n","# cosine resnet (last layer changed)\n","#from models.cosine_ResNet import *\n","\n","from models.iCaRL import *\n","from utils.utils import *\n","from copy import deepcopy\n","\n","import numpy as np\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NT0Un92iJHFo","colab_type":"text"},"source":["**GLOBAL PARAMETERS**"]},{"cell_type":"code","metadata":{"id":"eSHzmwaxJGEG","colab_type":"code","colab":{}},"source":["DEVICE = 'cuda'\n","NUM_CLASSES = 100\n","DATA_DIR = './CIFAR_100'\n","RUNS_DIR = '/content/Incremental-learning-on-image-recognition/RUNS'\n","\n","\n","# --- CUSTOM PARAMETERS\n","RANDOM_STATE = 2000          # int or None (Tarantino: 'tarantino', iCaRL: 1993, Telegram: 'telegram')\n","\n","N_GROUPS_FOR_TRAINING = 10   # Numero di gruppi di classi da usare in fase di training (1: usa solo il primo gruppo, 10: usa tutti i gruppi di classi)\n","\n","USE_HERDING = True\n","\n","GITHUB_USER = 2             # 0: Roberto, 1: Alessandro, 2: Gabriele\n","\n","CIFAR_NORMALIZE = False     # If True normalizes tensor with mean and standard deviation of CIFAR 100\n","\n","# VARIATION DYNAMIC K\n","STARTING_K = 2500\n","K_FUNCTION = 'boost'               # 'linear', 'triangle', 'triangle_optimized' 'exponential' 'cosine' 'cosine_trasl1' 'boost'\n","BCE_VAR_K_DYNAMIC = 'classMetà'    # 'standard', 'class', 'classFurba', 'classMetà'\n","BOOST_UNTIL_INCLUDED = 2           # 1, 2, 3, 4 -> fino a quale batch incluso (che va da 0 a 9)\n","BOOST_FUNCTION = 'linear'          # 'step', 'linear'\n","\n","METHOD = str(BCE_VAR_K_DYNAMIC)+'_'+str(STARTING_K)+'_'+str(K_FUNCTION)+' _ '+str(BOOST_FUNCTION)+' '+str(BOOST_UNTIL_INCLUDED)\n","# ---------------------\n","\n","\n","\n","\n","\n","# DA NON TOCCARE -------------------\n","DATA_AUGMENTATION = True\n","USE_VALIDATION_SET = False\n","SHUFFLE_CLASSES = True\n","DUMP_FINAL_RESULTS_ON_GSPREADSHEET = True\n","COMMIT_ON_GITHUB = True\n","EVAL_AFTER_EACH_EPOCH = False\n","BCE_VAR = 2          # 1: solo le classi attuali per il one-hot (loss divisa per 128x10, poi 128x20, etc.)\n","                     # 2: usa 100 classi fin da subito nel calcolo della loss (loss divisa sempre per 128x100)\n","                     # 3: usa le classi attuali per il one-hot ma dividi per 128x100 la loss\n","# ----------------------------------\n","\n","# --- HYPERPARAMETERS (NON TOCCARE)\n","# K = 2000\n","BATCH_SIZE = 128\n","LR = 2.                     # iCaRL uses LR=2 solo perchè usa la BCE, in generale usare 0.2\n","MOMENTUM = 0.9              # Hyperparameter for SGD, keep this at 0.9 when using SGD\n","WEIGHT_DECAY = 1e-5         # Regularization\n","\n","NUM_EPOCHS = 70             # Total number of training epochs (iterations over dataset)\n","DO_MULTILR_STEP_DOWN = True # step down at 7/10 and 9/10\n","STEP_SIZE = 10\n","GAMMA = 0.2                 # Multiplicative factor for learning rate step-down\n","# ---------------------"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"As6ukzaJE0kN","colab_type":"text"},"source":["**Define Data Preprocessing**"]},{"cell_type":"code","metadata":{"id":"e19l7N4HE6EB","colab_type":"code","colab":{}},"source":["if CIFAR_NORMALIZE: \n","  MEANS, STDS = (0.5071, 0.4865, 0.4409), (0.2673, 0.2564, 0.2762) # Normalizes tensor with mean and standard deviation of CIFAR 100\n","else: \n","  MEANS, STDS = (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)\n","\n","# Define transforms for training phase\n","if DATA_AUGMENTATION:\n","\ttrain_transform = transforms.Compose([\n","\t\t\t\t\t\t\t\t\t\t\ttransforms.RandomCrop(32, padding=4),\n","\t\t\t\t\t\t\t\t\t\t\ttransforms.RandomHorizontalFlip(p=0.5),\n","\t\t\t\t\t\t\t\t\t\t\ttransforms.ToTensor(), # Turn PIL Image to torch.Tensor\n","\t\t\t\t\t\t\t\t\t\t\ttransforms.Normalize(mean=MEANS, std=STDS) # Normalizes tensor with mean and standard deviation of CIFAR 100\n","\t\t\t\t\t\t\t\t\t])\n","else:\n","\ttrain_transform = transforms.Compose([\n","\t\t\t\t\t\t\t\t\t\t\ttransforms.ToTensor(), # Turn PIL Image to torch.Tensor\n","\t\t\t\t\t\t\t\t\t\t\ttransforms.Normalize(mean=MEANS, std=STDS) # Normalizes tensor with mean and standard deviation of CIFAR 100\n","\t\t\t\t\t\t\t\t\t])\n","\n","test_transform = transforms.Compose([\n","\t\t\t\t\t\t\t\t\t\ttransforms.ToTensor(),\n","\t\t\t\t\t\t\t\t\t\ttransforms.Normalize(mean=MEANS, std=STDS) # Normalizes tensor with mean and standard deviation of CIFAR 100                                                                                                \n","\t\t\t\t\t\t\t\t])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AVnFkK3mRhZG","colab_type":"text"},"source":["**Import dataset CIFAR-100**"]},{"cell_type":"code","metadata":{"id":"r_acReX5Rhkh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":69},"outputId":"ca386bdf-69c2-4e63-ba9e-6c18fc3f5e11"},"source":["#For any information about CIFAR-100 follow the link below\n","#https://www.cs.toronto.edu/~kriz/cifar.html\n","\n","train_dataset = CIFAR100(DATA_DIR, train=True, download=True, transform=train_transform)\n","test_dataset = CIFAR100(DATA_DIR, train=False, download=False, transform=test_transform)\n","\n","if SHUFFLE_CLASSES:\n","  # --- Shuffle class ordering\n","  if RANDOM_STATE == 'telegram':\n","    classes_indexes = np.array([30,  4, 36, 47, 81, 65, 66, 64, 68, 23, 72, 48, 54, 73,  6, 50, 51,\n","                          83, 75, 88, 58, 62, 39, 60, 94, 25, 84, 37, 33, 76, 34, 57, 46,  3,\n","                          24, 67, 17, 79, 40, 77, 26, 27, 41, 90, 89, 59, 20, 11, 61, 13, 44,\n","                          56,  9, 96, 70, 99, 82, 78,  5, 53, 16, 29,  0, 31,  7, 74, 55, 19,\n","                          42,  1, 92, 63, 52, 69, 22, 18, 28, 35,  8, 91, 86, 32, 97, 98, 15,\n","                            2, 45, 49, 95, 71, 14, 87, 80, 21, 38, 93, 43, 10, 12, 85])\n","    \n","  elif RANDOM_STATE == 'tarantino':\n","    random.seed(653)\n","    classes_indexes = [i for i in range(NUM_CLASSES)]\n","\n","    classes_indexes_cum = []\n","    remaining = [i for i in range(NUM_CLASSES)]\n","    for i in range(10):\n","      classes_indexes_cum += random.sample(remaining, 10)\n","      remaining = list(set(classes_indexes)-set(classes_indexes_cum))\n","\n","    classes_indexes = classes_indexes_cum\n","    classes_indexes = np.array(classes_indexes)\n","\n","    print('Tarantino classes order:', classes_indexes)\n","\n","  else:\n","    if RANDOM_STATE is not None:\n","      np.random.seed(RANDOM_STATE)\n","\n","    classes_indexes = np.array([i for i in range(NUM_CLASSES)])\n","    np.random.shuffle(classes_indexes)\n","\n","\n","  classes_shuffle_dict = {ind:i for i, ind in enumerate(classes_indexes)}\n","\n","  train_dataset.targets = [classes_shuffle_dict[tar] for tar in train_dataset.targets]\n","  test_dataset.targets = [classes_shuffle_dict[tar] for tar in test_dataset.targets]\n","\n","  CLASSES = train_dataset.classes\n","  train_dataset.class_to_idx = {CLASSES[i]:ind for i,ind in enumerate(classes_indexes)}\n","  LABEL_INDEX_DICT = train_dataset.class_to_idx\n","else:\n","  CLASSES = train_dataset.classes\n","  LABEL_INDEX_DICT = train_dataset.class_to_idx\n","\n","# show_random_images(train_dataset, 5, mean=MEANS, std=STDS)\n","\n","print('Train Dataset length:', len(train_dataset))\n","print('Test Dataset length:', len(test_dataset))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Files already downloaded and verified\n","Train Dataset length: 50000\n","Test Dataset length: 10000\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"C4ZhjI56LOIt","colab_type":"text"},"source":["**Prepare training**"]},{"cell_type":"code","metadata":{"id":"f70xiaS4LNo6","colab_type":"code","colab":{}},"source":["net = resnet32(num_classes=NUM_CLASSES)\n","icarl = iCaRL(device=DEVICE, batch_size=BATCH_SIZE, K=STARTING_K, dataset=train_dataset)\n","\n","# Define loss function\n","criterion = nn.BCEWithLogitsLoss(reduction='mean') # reduction='sum' is crucial as BCE is designed for one output neuron only (it averages on batch_size*num_classes instead of on just batch_size) - actually this is why iCaRL keeps a really high learning rate\n","criterion_eval = None"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8vJ7NW-_Ckuu","colab_type":"text"},"source":["**DEBUG**"]},{"cell_type":"code","metadata":{"id":"Zepx4GG9CkQZ","colab_type":"code","colab":{}},"source":["def get_new_num_exemplars(step):\n","  # Step che va da 0 a 9\n","\n","  # return (4000-(444*step)) # Linear da 4000 a 0\n","  # return (3000-(222*step)) # Linear da 3000 a 1000\n","\n","  if K_FUNCTION == 'linear':\n","    ending_k = 2000-(STARTING_K-2000)\n","    m = (STARTING_K-ending_k)/9\n","\n","    return round(STARTING_K-(m*step))\n","\n","  elif K_FUNCTION == 'triangle':\n","    vertex = 2000+(2000-STARTING_K)\n","    if step < 4.5:\n","      m = (vertex-STARTING_K)/4.5\n","      return STARTING_K+(m*step)\n","    else:\n","      m = -(vertex-STARTING_K)/4.5\n","      q = vertex-(4.5*m)\n","      return q+(m*step)\n","\n","  elif K_FUNCTION == 'triangle_optimized': #Fa si che la media sia davvero più vicina a 2000 essendo la funzione discreta (skew con il vertice in 4 invece che in 4.5)\n","      vertex = 2000+(2000-STARTING_K)\n","      if step <= 4:\n","        m = (vertex-STARTING_K)/4\n","        return STARTING_K+(m*step)\n","      else:\n","        m = -(vertex-STARTING_K)/5\n","        q = vertex-(4*m)\n","        return q+(m*step)\n","\n","  elif K_FUNCTION == 'exponential':\n","    supported_initial_k = [3500, 3000, 2500]\n","    alpha_values = {3500: 0.1385, 3000: 0.097, 2500: 0.0516}\n","    if STARTING_K not in supported_initial_k:\n","      raise RuntimeError('Non è possibile iniziare con questo valore di K')\n","\n","    alpha = alpha_values[STARTING_K]\n","    return STARTING_K*math.exp(-alpha*step)\n","\n","  elif K_FUNCTION == 'cosine':\n","    return ((STARTING_K-2000)*math.cos(2*math.pi/18*step))+2000\n","    \n","  elif K_FUNCTION == 'cosine_trasl1':\n","    supported_initial_k = [3000, 2500]\n","    altezza_curva = {3000: 1448.6, 2500: 1724.3}\n","    if STARTING_K not in supported_initial_k:\n","      raise RuntimeError('Non è possibile iniziare con questo valore di K con cosine_trasl1')\n","\n","    return ((STARTING_K-2000)*math.cos(2*math.pi/18*(step-3)))+altezza_curva[STARTING_K]\n","  \n","  elif K_FUNCTION == 'boost':\n","    if BOOST_FUNCTION == 'step':\n","      # BOOST_RESOURCE_PERCENTAGE = 0.5\n","      # BOOST_UNTIL_INCLUDED = 2\n","      boost_resource_percentage = (STARTING_K/2000)-1\n","      print('Resources overhead (percentage):', '+'+str(float(boost_resource_percentage/(10/(BOOST_UNTIL_INCLUDED+1))   )     )   )\n","      if step > BOOST_UNTIL_INCLUDED:\n","        return 2000\n","      else:\n","        return 2000+(2000*boost_resource_percentage)\n","\n","    elif BOOST_FUNCTION == 'linear':\n","      m = (STARTING_K-2000)/BOOST_UNTIL_INCLUDED\n","\n","      # Calcolo % resource overhead\n","      numExemplars = list()\n","      for i in range(10):\n","        if i > BOOST_UNTIL_INCLUDED:\n","          numExemplars.append(2000)\n","        else:\n","          numExemplars.append(STARTING_K-m*i)\n","        \n","      print('Resources overhead (percentage):', '+'+str( (float(np.mean(numExemplars))/2000)-1   ))\n","\n","      if step > BOOST_UNTIL_INCLUDED:\n","        return 2000\n","      else:\n","        return STARTING_K-m*step\n","    else:\n","      raise RuntimeError('Inserire una boost function valida')\n","  else:\n","    raise RuntimeError(\"Not yet supported\")\n","\n","# for i in range(10):\n","#   print(get_new_num_exemplars(i))\n","\n","# sys.exit()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yFXEmHyHo-Zt","colab_type":"text"},"source":["**Training**"]},{"cell_type":"code","metadata":{"id":"vaHEiGnro_85","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":712},"outputId":"1eec5e63-d9e1-4de7-affd-f1465b55a90e"},"source":["cudnn.benchmark # Calling this optimizes runtime\n","\n","val_indexes_cum = []\n","test_indexes_cum = []\n","current_classes_cum = []\n","\n","group_losses_train = []\n","group_losses_eval = []\n","group_accuracies_train = []\n","group_accuracies_eval = []\n","group_accuracies_eval_curr = []\n","group_accuracies_eval_nme = []\n","\n","now = datetime.datetime.now(timezone('Europe/Rome'))\n","CURRENT_RUN = 'RUN_' + now.strftime(\"%Y-%m-%d %H %M %S\")\n","try:\n","  os.makedirs(RUNS_DIR+'/'+CURRENT_RUN)\n","except OSError:\n","  print (\"FATAL ERROR - Creation of the directory of the current run failed\")\n","  sys.exit()\n","\n","dump_hyperparameters(path=RUNS_DIR+'/'+CURRENT_RUN, lr=LR, weight_decay=WEIGHT_DECAY, num_epochs=NUM_EPOCHS, method=METHOD, batch_size=BATCH_SIZE)\n","\n","START_TIME = time.time()\n","\n","for group_number in range(N_GROUPS_FOR_TRAINING):\n","\n","  new_K_value = get_new_num_exemplars(group_number)\n","\n","  if group_number > 0:\n","    # Check if constraint on number of exemplars has been satisfied\n","    old_num_ex_per_batch = icarl.K/(group_number-1+1)\n","    curr_num_ex_per_batch = new_K_value/(group_number+1)\n","\n","    if curr_num_ex_per_batch > old_num_ex_per_batch:\n","      raise RuntimeError('Non è stata soddisfatta la constraint sul massimo numero di exemplars da aggiungere a questo step')\n","\n","  icarl.K = new_K_value\n","  print('**** CURRENT NUM EXEMPLARS:', icarl.K)\n","\n","  starting_label = (group_number*10)\n","  ending_label = (group_number+1)*10\n","  current_classes = list(range(starting_label, ending_label))\n","\n","  new_indexes = get_indexes_from_labels(train_dataset, current_classes)\n","\n","  # np.random.shuffle(new_indexes)\n","\n","  train_dataset_curr = Subset(train_dataset, new_indexes)\n","  exemplars = icarl.flattened_exemplars()\n","  train_dataset_cum_exemplars = Subset(train_dataset, exemplars+new_indexes)\n","\n","  # Update training set\n","  train_dataloader = DataLoader(train_dataset_curr, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n","  train_dataloader_cum_exemplars = DataLoader(train_dataset_cum_exemplars, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n","\n","  train_dataloader_for_evaluation = DataLoader(train_dataset_curr, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n","  train_dataloader_cum_exemplars_for_evaluation = DataLoader(train_dataset_cum_exemplars, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n","\n","  # Update test set\n","  new_test_indexes = get_indexes_from_labels(test_dataset, current_classes)\n","  test_dataset_cum = Subset(test_dataset, test_indexes_cum+new_test_indexes)\n","  test_dataset_curr = Subset(test_dataset, new_test_indexes)\n","\n","  test_indexes_cum += new_test_indexes\n","\n","  test_dataloader = DataLoader(test_dataset_cum, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n","  test_dataloader_curr = DataLoader(test_dataset_curr, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n","\n","  print('******************************')\n","  print(f'NEW GROUP OF CLASSES {(group_number+1)}°/{N_GROUPS_FOR_TRAINING}')\n","  print('Training set length:', len(train_dataset_curr))\n","  if USE_VALIDATION_SET:\n","    print('Validation set length:', len(val_dataset_cum))\n","  print('Test set length:', len(test_dataset_cum))\n","  \n","  net = net.to(DEVICE)\n","\n","  parameters_to_optimize = net.parameters()\n","\n","  optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n","  milestone_1 = math.floor(NUM_EPOCHS/10*7)\n","  milestone_2 = math.floor(NUM_EPOCHS/10*9)\n","\n","  scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[milestone_1, milestone_2], gamma=GAMMA)\n","\n","  current_step = 0\n","  losses_train = []\n","  losses_eval = []\n","  accuracies_train = []\n","  accuracies_eval = []\n","  accuracies_eval_curr = []\n","  accuracies_eval_nme = []\n","\n","  net_old = None\n","  if starting_label > 0:\n","    # Salva la rete attuale per calcolare i vecchi outputs\n","    net_old = deepcopy(net)\n","\n","  net.train()\n","  for epoch in range(NUM_EPOCHS):\n","    print('Starting epoch {}/{}, LR = {}'.format(epoch+1, NUM_EPOCHS, scheduler.get_lr()))\n","\n","    #\n","    # Update weights using iCaRL BCE and distillation loss on Dataset\n","    #\n","    loss = icarl.update_representation(net, net_old, train_dataloader_cum_exemplars, criterion, optimizer, current_classes, starting_label, ending_label, current_step, bce_var=BCE_VAR, k_dinamico=True, k_dinamico_var=BCE_VAR_K_DYNAMIC, boost_until_included=BOOST_UNTIL_INCLUDED)\n","\n","    current_step += 1\n","    scheduler.step()\n","\n","    print('--- Epoch {}, Loss on train: {}'.format(epoch+1, loss.item()))\n","    losses_train.append(loss.item())\n","\n","\n","  # --- END OF TRAINING FOR THIS GROUP OF CLASSES\n","  print('Length on train dataset (exemplars included):', len(train_dataset_cum_exemplars))\n","\n","  #\n","  # Compute means of each class using the entire current training set and the exemplars\n","  #\n","  icarl.compute_means(net, train_dataloader_cum_exemplars, ending_label)\n","\n","  if starting_label > 0:\n","    #\n","    # Reduce number of exemplars for each class to 2000/ending_label\n","    #\n","    icarl.reduce_exemplars(starting_label, ending_label)\n"," \n","  #\n","  # Construct exemplars for future evaluation\n","  #\n","  icarl.construct_exemplars(net, starting_label, ending_label, herding=USE_HERDING)\n","\n","  with torch.no_grad():\n","    _, accuracy_test = eval_model(net, test_dataloader, criterion=criterion_eval,\n","                                          dataset_length=len(test_dataset_cum), use_bce_loss=None,\n","                                          ending_label=ending_label, loss=False, device=DEVICE, display=True, suffix=' (group)')\n","  losses_eval.append(-1)\n","  accuracies_eval.append(accuracy_test)\n","\n","  #\n","  # Eval model using NME on test set\n","  #\n","  with torch.no_grad():\n","    accuracy_eval_nme = icarl.eval_model_nme(net, test_dataloader, dataset_length=len(test_dataset_cum), display=True, suffix=' (group)')\n","  accuracies_eval_nme.append(accuracy_eval_nme)\n","\n","  #\n","  # Accuracy on training\n","  #\n","  with torch.no_grad():\n","    accuracy_train = eval_model_accuracy(net, train_dataloader_for_evaluation, dataset_length=len(train_dataset_curr), starting_label=starting_label, ending_label=ending_label, device=DEVICE, display=True, suffix='train (group)')\n","  accuracies_train.append(accuracy_train)\n","\n","  #\n","  # Compute accuracy on test for novel classes only\n","  #\n","  with torch.no_grad():\n","    accuracy_eval_curr_classes = eval_model_accuracy(net, test_dataloader_curr, dataset_length=len(test_dataset_curr), starting_label=starting_label, ending_label=ending_label, device=DEVICE, display=True, suffix='test novel classes (group)')\n","  accuracies_eval_curr.append(accuracy_eval_curr_classes)\n","\n","  path = RUNS_DIR+'/'+CURRENT_RUN    \n","  create_dir_for_current_group(group_number, path=path)\n","  \n","  draw_graphs(losses_train,\n","        losses_eval,\n","        accuracies_train,\n","        accuracies_eval,\n","        num_epochs=NUM_EPOCHS, use_validation=USE_VALIDATION_SET, print_img=False, save=True, path=path, group_number=group_number)\n","  \n","  dump_to_csv(losses_train,\n","        losses_eval,\n","        accuracies_train,\n","        accuracies_eval,\n","        group_number=group_number, path=path)\n","\n","  group_losses_train.append(losses_train[-1])\n","  group_losses_eval.append(losses_eval[-1])\n","  group_accuracies_train.append(accuracies_train[-1])\n","  group_accuracies_eval.append(accuracies_eval[-1])\n","  group_accuracies_eval_curr.append(accuracies_eval_curr[-1])\n","  group_accuracies_eval_nme.append(accuracies_eval_nme[-1])\n","\n","# END OF OVERALL TRAINING\n","dump_final_values_nme(group_losses_train, group_accuracies_train, group_accuracies_eval_nme, group_accuracies_eval, group_accuracies_eval_curr, path=RUNS_DIR+'/'+CURRENT_RUN)\n","draw_final_graphs_nme(group_losses_train, group_accuracies_eval_nme, group_accuracies_eval, use_validation=USE_VALIDATION_SET, print_img=True, save=True, path=RUNS_DIR+'/'+CURRENT_RUN)\n","\n","print('Average incremental accuracy (nme)', np.mean(group_accuracies_eval_nme))\n","print('Average incremental accuracy (hybrid 1)', np.mean(group_accuracies_eval))\n","\n","#\n","# Compute and display confusion matrix\n","#\n","conf_mat = get_conf_matrix_nme(net, test_dataloader, icarl=icarl, ending_label=ending_label, device=DEVICE)\n","display_conf_matrix(conf_mat, display=True, save=True, path=RUNS_DIR+'/'+CURRENT_RUN)\n","\n","DURATION = round((time.time()-START_TIME)/60, 1)\n","print(f\"> In {(DURATION)} minutes\")\n","\n","github_link = 'https://github.com/gabrieletiboni/Incremental-learning-on-image-recognition/tree/master/RUNS/'+str(CURRENT_RUN)\n","github_link = github_link.replace(\" \", \"%20\")\n","hyperparameters_string = get_hyperparameter_string(lr=LR, weight_decay=WEIGHT_DECAY, num_epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, multilrstep=DO_MULTILR_STEP_DOWN, gamma=GAMMA)\n","if DUMP_FINAL_RESULTS_ON_GSPREADSHEET:\n","  dump_on_gspreadsheet_nme(CURRENT_RUN, GITHUB_USER, github_link, METHOD, RANDOM_STATE, USE_HERDING, CIFAR_NORMALIZE, BCE_VAR, group_losses_train, group_accuracies_train, group_accuracies_eval_nme, group_accuracies_eval, group_accuracies_eval_curr, DURATION, hyperparameters=hyperparameters_string)\n","\n","beep()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Resources overhead (percentage): +0.03750000000000009\n","**** CURRENT NUM EXEMPLARS: 2500.0\n","******************************\n","NEW GROUP OF CLASSES 1°/10\n","Training set length: 5000\n","Test set length: 1000\n","Starting epoch 1/70, LR = [2.0]\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:396: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  \"please use `get_last_lr()`.\", UserWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["--- Initial loss on train: 0.7563686966896057\n","--- Epoch 1, Loss on train: 0.02890641614794731\n","Starting epoch 2/70, LR = [2.0]\n","--- Epoch 2, Loss on train: 0.02288122847676277\n","Starting epoch 3/70, LR = [2.0]\n","--- Epoch 3, Loss on train: 0.02058003842830658\n","Starting epoch 4/70, LR = [2.0]\n","--- Epoch 4, Loss on train: 0.018644776195287704\n","Starting epoch 5/70, LR = [2.0]\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-121-638b6d0c6f98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;31m# Update weights using iCaRL BCE and distillation loss on Dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0micarl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_representation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet_old\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader_cum_exemplars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstarting_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mending_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbce_var\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBCE_VAR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_dinamico\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_dinamico_var\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBCE_VAR_K_DYNAMIC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboost_until_included\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0mcurrent_step\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/models/iCaRL.py\u001b[0m in \u001b[0;36mupdate_representation\u001b[0;34m(self, net, net_old, train_dataloader_cum_exemplars, criterion, optimizer, current_classes, starting_label, ending_label, current_step, bce_var, loss_type, alpha, k_dinamico, k_dinamico_var, boost_until_included)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mloss_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'bce'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbce_loss_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet_old\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstarting_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mending_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbce_var\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbce_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_dinamico\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk_dinamico\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_dinamico_var\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk_dinamico_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboost_until_included\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mboost_until_included\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mloss_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'ce_l2'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCE_L2_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet_old\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstarting_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mending_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistillation_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs_normalization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/models/iCaRL.py\u001b[0m in \u001b[0;36mbce_loss_with_logits\u001b[0;34m(self, net, net_old, criterion, images, labels, current_classes, starting_label, ending_label, bce_var, k_dinamico, k_dinamico_var, boost_until_included)\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0;31m# one hot encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m                 \u001b[0mtargets_bce\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[0mtargets_bce\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets_bce\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"W0zb4Vqe-sXo","colab_type":"code","colab":{}},"source":["if COMMIT_ON_GITHUB:\n","  if GITHUB_USER == 0:\n","    # Roberto\n","    GITHUB_CREDENTIALS_EMAIL = 'roberto.franceschi@studenti.polito.it'\n","    GITHUB_CREDENTIALS_USERNAME = 'robertofranceschi'\n","  elif GITHUB_USER == 1:\n","    # Alessandro\n","    GITHUB_CREDENTIALS_EMAIL = 'ale.dex95@gmail.com'\n","    GITHUB_CREDENTIALS_USERNAME = 'Deso95'\n","  elif GITHUB_USER == 2:\n","    # Gabriele\n","    GITHUB_CREDENTIALS_EMAIL = 'tibonigabriele@gmail.com'\n","    GITHUB_CREDENTIALS_USERNAME = 'gabrieletiboni'\n","  else:\n","    raise (RuntimeError('FATAL ERROR - Selezionare un GITHUB user da 0 a 2.'))\n","  \n","  GITHUB_ORIGIN = \"https://\"+GITHUB_CREDENTIALS_USERNAME+\":Github12345!@github.com/gabrieletiboni/Incremental-learning-on-image-recognition.git\"\n","\n","  !git config --global user.email \"$GITHUB_CREDENTIALS_EMAIL\"\n","  !git config --global user.name \"$GITHUB_CREDENTIALS_USERNAME\"\n","  %cd \"/content/Incremental-learning-on-image-recognition/\"\n","\n","  !git pull\n","\n","  #!git status\n","  !git remote set-url origin \"$GITHUB_ORIGIN\"\n","  !git add RUNS/\n","  #### IN ALTERNATIVA A \"add\" #!git rm -r \"./RUNS/$CURRENT_RUN/\"\n","  !git commit -m \"$CURRENT_RUN\"\n","\n","  !git push -u origin master"],"execution_count":null,"outputs":[]}]}