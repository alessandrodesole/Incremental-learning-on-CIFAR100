{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Finetuning_JointTraining.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGeu47FfPs5I",
        "colab_type": "text"
      },
      "source": [
        "### **Finetuning and Joint Training**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CwlqKl4RzLi",
        "colab_type": "text"
      },
      "source": [
        "**Install requirements**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9O3aM3Tb28q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip3 install 'torch==1.3.1'\n",
        "#!pip3 install 'torchvision==0.5.0'\n",
        "#!pip3 install 'Pillow-SIMD'\n",
        "#!pip3 install 'tqdm'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrkE-2MVbZjU",
        "colab_type": "text"
      },
      "source": [
        "**Import models and functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOGd-aASa40O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "if not os.path.isdir('./models'):\n",
        "  !git clone https://github.com/gabrieletiboni/Incremental-learning-on-image-recognition.git\n",
        "  !cp -r \"/content/Incremental-learning-on-image-recognition/CODE/utils\" \"/content/\"\n",
        "  !cp -r \"/content/Incremental-learning-on-image-recognition/CODE/models\" \"/content/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esZgBn2rBV9L",
        "colab_type": "text"
      },
      "source": [
        "**Import libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyDs3TD9Bda1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.backends import cudnn\n",
        "\n",
        "import datetime\n",
        "from pytz import timezone\n",
        "import os\n",
        "import math\n",
        "import time\n",
        "\n",
        "from torchvision.datasets import CIFAR100\n",
        "from torchvision.transforms import transforms\n",
        "import torchvision.utils\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "import torch.optim as optim\n",
        "\n",
        "from models.ResNet import resnet32\n",
        "from utils.utils import *\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NT0Un92iJHFo",
        "colab_type": "text"
      },
      "source": [
        "**GLOBAL PARAMETERS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSHzmwaxJGEG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DEVICE = 'cuda'\n",
        "NUM_CLASSES = 100\n",
        "DATA_DIR = './CIFAR_100'\n",
        "RUNS_DIR = '/content/Incremental-learning-on-image-recognition/RUNS'\n",
        "\n",
        "# --- CUSTOM PARAMETERS\n",
        "RANDOM_STATE = 2000          # int or None (Tarantino: 'tarantino', iCaRL: 1993, Telegram: 'telegram')\n",
        "\n",
        "N_GROUPS_FOR_TRAINING = 10   # Numero di gruppi di classi da usare in fase di training (1: usa solo il primo gruppo, 10: usa tutti i gruppi di classi)\n",
        "\n",
        "USE_BCE_LOSS = True\n",
        "\n",
        "GITHUB_USER = 2             # 0: Roberto, 1: Alessandro, 2: Gabriele\n",
        "\n",
        "CIFAR_NORMALIZE = True     # If True normalizes tensor with mean and standard deviation of CIFAR 100\n",
        "\n",
        "DO_JOINT_TRAINING = True\n",
        "\n",
        "METHOD = 'Joint_training_con_BCE_2000'\n",
        "# ---------------------\n",
        "\n",
        "DATA_AUGMENTATION = True\n",
        "USE_VALIDATION_SET = False\n",
        "SHUFFLE_CLASSES = True\n",
        "DUMP_FINAL_RESULTS_ON_GSPREADSHEET = True\n",
        "COMMIT_ON_GITHUB = True\n",
        "EVAL_AFTER_EACH_EPOCH = False\n",
        "\n",
        "# Force NO-BCE-LOSS when doing Joint Training\n",
        "# if DO_JOINT_TRAINING:\n",
        "#   USE_BCE_LOSS = False\n",
        "# ----------------------------------\n",
        "\n",
        "# --- HYPERPARAMETERS\n",
        "BATCH_SIZE = 128\n",
        "if USE_BCE_LOSS:\n",
        "  LR = 2\n",
        "else:\n",
        "  LR = 0.2                  # iCaRL uses LR=2 solo perch√® usa la BCE, in generale usare 0.2\n",
        "MOMENTUM = 0.9              # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
        "WEIGHT_DECAY = 1e-5         # Regularization\n",
        "\n",
        "NUM_EPOCHS = 70             # Total number of training epochs (iterations over dataset)\n",
        "DO_MULTILR_STEP_DOWN = True # step down at 7/10 and 9/10\n",
        "STEP_SIZE = 10\n",
        "GAMMA = 0.2                 # Multiplicative factor for learning rate step-down\n",
        "# ---------------------"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "As6ukzaJE0kN",
        "colab_type": "text"
      },
      "source": [
        "**Define Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e19l7N4HE6EB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if CIFAR_NORMALIZE: \n",
        "  MEANS, STDS = (0.5071, 0.4865, 0.4409), (0.2673, 0.2564, 0.2762) # Normalizes tensor with mean and standard deviation of CIFAR 100\n",
        "else: \n",
        "  MEANS, STDS = (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)\n",
        "\n",
        "# Define transforms for training phase\n",
        "if DATA_AUGMENTATION:\n",
        "  train_transform = transforms.Compose([\n",
        "                                            transforms.RandomCrop(32, padding=4),\n",
        "                                            transforms.RandomHorizontalFlip(p=0.5),\n",
        "                                            transforms.ToTensor(), # Turn PIL Image to torch.Tensor\n",
        "                                            transforms.Normalize(mean=MEANS, std=STDS) # Normalizes tensor with mean and standard deviation of CIFAR 100\n",
        "                                        ])\n",
        "else:\n",
        "  train_transform = transforms.Compose([\n",
        "                                        transforms.ToTensor(), # Turn PIL Image to torch.Tensor\n",
        "                                        transforms.Normalize(mean=MEANS, std=STDS) # Normalizes tensor with mean and standard deviation of CIFAR 100\n",
        "                                    ])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "                                        transforms.ToTensor(),\n",
        "                                        transforms.Normalize(mean=MEANS, std=STDS) # Normalizes tensor with mean and standard deviation of CIFAR 100                                                                                                \n",
        "                                    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVnFkK3mRhZG",
        "colab_type": "text"
      },
      "source": [
        "**Import dataset CIFAR-100**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_acReX5Rhkh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        },
        "outputId": "aa4e1d79-e501-4f84-d86f-66261d0b1b51"
      },
      "source": [
        "#For any information about CIFAR-100 follow the link below\n",
        "#https://www.cs.toronto.edu/~kriz/cifar.html\n",
        "\n",
        "train_dataset = CIFAR100(DATA_DIR, train=True, download=True, transform=train_transform)\n",
        "test_dataset = CIFAR100(DATA_DIR, train=False, download=False, transform=test_transform)\n",
        "\n",
        "if SHUFFLE_CLASSES:\n",
        "  # --- Shuffle class ordering\n",
        "  if RANDOM_STATE == 'telegram':\n",
        "    classes_indexes = np.array([30,  4, 36, 47, 81, 65, 66, 64, 68, 23, 72, 48, 54, 73,  6, 50, 51,\n",
        "                          83, 75, 88, 58, 62, 39, 60, 94, 25, 84, 37, 33, 76, 34, 57, 46,  3,\n",
        "                          24, 67, 17, 79, 40, 77, 26, 27, 41, 90, 89, 59, 20, 11, 61, 13, 44,\n",
        "                          56,  9, 96, 70, 99, 82, 78,  5, 53, 16, 29,  0, 31,  7, 74, 55, 19,\n",
        "                          42,  1, 92, 63, 52, 69, 22, 18, 28, 35,  8, 91, 86, 32, 97, 98, 15,\n",
        "                            2, 45, 49, 95, 71, 14, 87, 80, 21, 38, 93, 43, 10, 12, 85])\n",
        "    \n",
        "  elif RANDOM_STATE == 'tarantino':\n",
        "    random.seed(653)\n",
        "    classes_indexes = [i for i in range(NUM_CLASSES)]\n",
        "\n",
        "    classes_indexes_cum = []\n",
        "    remaining = [i for i in range(NUM_CLASSES)]\n",
        "    for i in range(10):\n",
        "      classes_indexes_cum += random.sample(remaining, 10)\n",
        "      remaining = list(set(classes_indexes)-set(classes_indexes_cum))\n",
        "\n",
        "    classes_indexes = classes_indexes_cum\n",
        "    classes_indexes = np.array(classes_indexes)\n",
        "\n",
        "    print('Tarantino classes order:', classes_indexes)\n",
        "\n",
        "  else:\n",
        "    if RANDOM_STATE is not None:\n",
        "      np.random.seed(RANDOM_STATE)\n",
        "\n",
        "    classes_indexes = np.array([i for i in range(NUM_CLASSES)])\n",
        "    np.random.shuffle(classes_indexes)\n",
        "\n",
        "\n",
        "  classes_shuffle_dict = {ind:i for i, ind in enumerate(classes_indexes)}\n",
        "\n",
        "  train_dataset.targets = [classes_shuffle_dict[tar] for tar in train_dataset.targets]\n",
        "  test_dataset.targets = [classes_shuffle_dict[tar] for tar in test_dataset.targets]\n",
        "\n",
        "  CLASSES = train_dataset.classes\n",
        "  train_dataset.class_to_idx = {CLASSES[i]:ind for i,ind in enumerate(classes_indexes)}\n",
        "  LABEL_INDEX_DICT = train_dataset.class_to_idx\n",
        "else:\n",
        "  CLASSES = train_dataset.classes\n",
        "  LABEL_INDEX_DICT = train_dataset.class_to_idx\n",
        "\n",
        "# show_random_images(train_dataset, 5, mean=MEANS, std=STDS)\n",
        "\n",
        "print('Train Dataset length:', len(train_dataset))\n",
        "print('Test Dataset length:', len(test_dataset))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Train Dataset length: 50000\n",
            "Test Dataset length: 10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4ZhjI56LOIt",
        "colab_type": "text"
      },
      "source": [
        "**Prepare training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f70xiaS4LNo6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = resnet32(num_classes=NUM_CLASSES)\n",
        "\n",
        "# Define loss function\n",
        "if USE_BCE_LOSS:\n",
        "  criterion = nn.BCEWithLogitsLoss() # SUM is crucial as BCE is designed for one output neuron only (it averages on batch_size*num_classes instead of on just batch_size)\n",
        "  criterion_eval = nn.BCEWithLogitsLoss()\n",
        "else:\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  criterion_eval = nn.CrossEntropyLoss(reduction='sum')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFXEmHyHo-Zt",
        "colab_type": "text"
      },
      "source": [
        "**Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaHEiGnro_85",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bd688eb-018e-4709-b42a-308ca6b8c392"
      },
      "source": [
        "cudnn.benchmark # Calling this optimizes runtime\n",
        "\n",
        "val_indexes_cum = []\n",
        "test_indexes_cum = []\n",
        "current_classes_cum = []\n",
        "\n",
        "group_losses_train = []\n",
        "group_losses_eval = []\n",
        "group_accuracies_train = []\n",
        "group_accuracies_eval = []\n",
        "group_accuracies_eval_curr = []\n",
        "\n",
        "now = datetime.datetime.now(timezone('Europe/Rome'))\n",
        "CURRENT_RUN = 'RUN_' + now.strftime(\"%Y-%m-%d %H %M %S\")\n",
        "try:\n",
        "  os.makedirs(RUNS_DIR+'/'+CURRENT_RUN)\n",
        "except OSError:\n",
        "  print (\"FATAL ERROR - Creation of the directory of the current run failed\")\n",
        "  sys.exit()\n",
        "\n",
        "dump_hyperparameters(path=RUNS_DIR+'/'+CURRENT_RUN, lr=LR, weight_decay=WEIGHT_DECAY, num_epochs=NUM_EPOCHS, method=METHOD, batch_size=BATCH_SIZE)\n",
        "\n",
        "START_TIME = time.time()\n",
        "\n",
        "for group_number in range(1, N_GROUPS_FOR_TRAINING):\n",
        "\n",
        "  if DO_JOINT_TRAINING:\n",
        "    net = resnet32(num_classes=NUM_CLASSES)\n",
        "    starting_label = 0\n",
        "  else:\n",
        "    starting_label = (group_number*10)\n",
        "\n",
        "  ending_label = (group_number+1)*10\n",
        "  current_classes = list(range(starting_label, ending_label))\n",
        "\n",
        "  new_indexes = get_indexes_from_labels(train_dataset, current_classes)\n",
        "\n",
        "  if USE_VALIDATION_SET:\n",
        "    new_train_indexes, new_val_indexes = train_validation_split(train_dataset, new_indexes, train_size=0.9, random_state=RANDOM_STATE)\n",
        "\n",
        "    train_dataset_curr = Subset(train_dataset, new_train_indexes)\n",
        "    val_dataset_cum = Subset(train_dataset, val_indexes_cum+new_val_indexes)\n",
        "    val_dataset_curr = Subset(train_dataset, new_val_indexes)\n",
        "\n",
        "    val_dataloader = DataLoader(val_dataset_cum, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "    val_dataloader_curr = DataLoader(val_dataset_curr, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "\n",
        "    val_indexes_cum += new_val_indexes\n",
        "\n",
        "  else:\n",
        "    train_dataset_curr = Subset(train_dataset, new_indexes)\n",
        "\n",
        "  # Update training set\n",
        "  train_dataloader = DataLoader(train_dataset_curr, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
        "  train_dataloader_for_evaluation = DataLoader(train_dataset_curr, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "\n",
        "  # Update test set\n",
        "  new_test_indexes = get_indexes_from_labels(test_dataset, current_classes)\n",
        "  test_dataset_cum = Subset(test_dataset, test_indexes_cum+new_test_indexes)\n",
        "  test_dataset_curr = Subset(test_dataset, new_test_indexes)\n",
        "  if DO_JOINT_TRAINING == False:\n",
        "    test_indexes_cum += new_test_indexes\n",
        "\n",
        "  test_dataloader = DataLoader(test_dataset_cum, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "  test_dataloader_curr = DataLoader(test_dataset_curr, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "\n",
        "  print('******************************')\n",
        "  print(f'NEW GROUP OF CLASSES {(group_number+1)}¬∞/{N_GROUPS_FOR_TRAINING}')\n",
        "  print('Training set length:', len(train_dataset_curr))\n",
        "  if USE_VALIDATION_SET:\n",
        "    print('Validation set length:', len(val_dataset_cum))\n",
        "  print('Test set length:', len(test_dataset_cum))\n",
        "  \n",
        "  net = net.to(DEVICE)\n",
        "\n",
        "  parameters_to_optimize = net.parameters()\n",
        "\n",
        "  optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "  if DO_MULTILR_STEP_DOWN:\n",
        "    milestone_1 = math.floor(NUM_EPOCHS/10*7)\n",
        "    milestone_2 = math.floor(NUM_EPOCHS/10*9)\n",
        "\n",
        "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[milestone_1, milestone_2], gamma=GAMMA)\n",
        "  else:\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)\n",
        "\n",
        "  current_step = 0\n",
        "  losses_train = []\n",
        "  losses_eval = []\n",
        "  accuracies_train = []\n",
        "  accuracies_eval = []\n",
        "  accuracies_eval_curr = []\n",
        "\n",
        "  for epoch in range(NUM_EPOCHS):\n",
        "    print('Starting epoch {}/{}, LR = {}'.format(epoch+1, NUM_EPOCHS, scheduler.get_lr()))\n",
        "\n",
        "    # Iterate over the dataset\n",
        "    for images, labels in train_dataloader:\n",
        "      # Bring data over the device of choice\n",
        "      images = images.to(DEVICE)\n",
        "      labels = labels.to(DEVICE)\n",
        "\n",
        "      net.train() # Sets module in training mode\n",
        "\n",
        "      optimizer.zero_grad() # Zero-ing the gradients\n",
        "\n",
        "      # Forward pass to the network\n",
        "      outputs = net(images)\n",
        "\n",
        "      if USE_BCE_LOSS:\n",
        "        targets_bce = torch.zeros([BATCH_SIZE, 100], dtype=torch.float32)\n",
        "\n",
        "        for i in range(BATCH_SIZE):\n",
        "          targets_bce[i][labels[i]] = 1\n",
        "\n",
        "        targets_bce = targets_bce.to(DEVICE)\n",
        "\n",
        "        loss = criterion(outputs[:, 0:100], targets_bce)\n",
        "\n",
        "      else:\n",
        "        loss = criterion(outputs[:, 0:ending_label], labels)\n",
        "      \n",
        "      if current_step == 0:\n",
        "        print('--- Initial loss on train: {}'.format(loss.item()))\n",
        "\n",
        "      # Compute gradients for each layer and update weights\n",
        "      loss.backward()  # backward pass: computes gradients\n",
        "      optimizer.step() # update weights based on accumulated gradients\n",
        "\n",
        "      current_step += 1\n",
        "\n",
        "    # --- END OF CURRENT EPOCH \n",
        "    scheduler.step()\n",
        "\n",
        "    print('--- Epoch {}, Loss on train: {}'.format(epoch+1, loss.item()))\n",
        "    losses_train.append(loss.item())\n",
        "\n",
        "\n",
        "  # --- END OF TRAINING FOR THIS GROUP OF CLASSES\n",
        "  if EVAL_AFTER_EACH_EPOCH == False: # Do it at least at the end of each group\n",
        "    \n",
        "    if USE_VALIDATION_SET:\n",
        "      with torch.no_grad():\n",
        "        loss_val, accuracy_val = eval_model(net, val_dataloader, criterion=criterion_eval,\n",
        "                                            dataset_length=len(val_dataset_cum), use_bce_loss=USE_BCE_LOSS,\n",
        "                                            ending_label=ending_label, device=DEVICE, display=True, suffix=' (group)')\n",
        "\n",
        "      losses_eval.append(loss_val)\n",
        "      accuracies_eval.append(accuracy_val)\n",
        "    else:\n",
        "      with torch.no_grad():\n",
        "        loss_test, accuracy_test = eval_model(net, test_dataloader, criterion=criterion_eval,\n",
        "                                              dataset_length=len(test_dataset_cum), use_bce_loss=USE_BCE_LOSS,\n",
        "                                              ending_label=ending_label, device=DEVICE, display=True, suffix=' (group)')\n",
        "\n",
        "      losses_eval.append(loss_test)\n",
        "      accuracies_eval.append(accuracy_test)\n",
        "\n",
        "  # --- Accuracy on training\n",
        "  with torch.no_grad():\n",
        "    accuracy_train = eval_model_accuracy(net, train_dataloader_for_evaluation, dataset_length=len(train_dataset_curr), starting_label=starting_label, ending_label=ending_label, device=DEVICE, display=True, suffix='train (group)')\n",
        "  \n",
        "  accuracies_train.append(accuracy_train)\n",
        "  # ------------------------\n",
        "\n",
        "  # --- Compute accuracy on test for novel classes only\n",
        "  if DO_JOINT_TRAINING == False:\n",
        "    with torch.no_grad():\n",
        "      accuracy_eval_curr_classes = eval_model_accuracy(net, test_dataloader_curr, dataset_length=len(test_dataset_curr), starting_label=starting_label, ending_label=ending_label, device=DEVICE, display=True, suffix='test novel classes (group)')\n",
        "    accuracies_eval_curr.append(accuracy_eval_curr_classes)\n",
        "  # ---------------------------------------------------\n",
        "\n",
        "  path = RUNS_DIR+'/'+CURRENT_RUN    \n",
        "  create_dir_for_current_group(group_number, path=path)\n",
        "  \n",
        "  draw_graphs(losses_train,\n",
        "        losses_eval,\n",
        "        accuracies_train,\n",
        "        accuracies_eval,\n",
        "        num_epochs=NUM_EPOCHS, use_validation=USE_VALIDATION_SET, print_img=False, save=True, path=path, group_number=group_number)\n",
        "  \n",
        "  dump_to_csv(losses_train,\n",
        "        losses_eval,\n",
        "        accuracies_train,\n",
        "        accuracies_eval,\n",
        "        group_number=group_number, path=path)\n",
        "\n",
        "  group_losses_train.append(losses_train[-1])\n",
        "  group_losses_eval.append(losses_eval[-1])\n",
        "  group_accuracies_train.append(accuracies_train[-1])\n",
        "  group_accuracies_eval.append(accuracies_eval[-1])\n",
        "  if DO_JOINT_TRAINING == False:\n",
        "    group_accuracies_eval_curr.append(accuracies_eval_curr[-1])\n",
        "\n",
        "# END OF OVERALL TRAINING\n",
        "dump_final_values(group_losses_train, group_losses_eval, group_accuracies_train, group_accuracies_eval, group_accuracies_eval_curr, path=RUNS_DIR+'/'+CURRENT_RUN)\n",
        "draw_final_graphs(group_losses_train, group_losses_eval, group_accuracies_eval_curr, group_accuracies_eval, DO_JOINT_TRAINING, use_validation=USE_VALIDATION_SET, print_img=True, save=True, path=RUNS_DIR+'/'+CURRENT_RUN)\n",
        "\n",
        "# --- Compute and display confusion matrix\n",
        "if USE_VALIDATION_SET:\n",
        "  conf_mat = get_conf_matrix(net, val_dataloader, ending_label=ending_label, device=DEVICE)\n",
        "else:\n",
        "  conf_mat = get_conf_matrix(net, test_dataloader, ending_label=ending_label, device=DEVICE)\n",
        "display_conf_matrix(conf_mat, display=True, save=True, path=RUNS_DIR+'/'+CURRENT_RUN)\n",
        "# --- \n",
        "\n",
        "DURATION = round((time.time()-START_TIME)/60, 1)\n",
        "print(f\"> In {(DURATION)} minutes\")\n",
        "\n",
        "#todo: github commit of local directories\n",
        "#todo: hyperparameters string in dump_on_gspreadsheet\n",
        "github_link = 'https://github.com/gabrieletiboni/Incremental-learning-on-image-recognition/tree/master/RUNS/'+str(CURRENT_RUN)\n",
        "github_link = github_link.replace(\" \", \"%20\")\n",
        "hyperparameters_string = get_hyperparameter_string(lr=LR, weight_decay=WEIGHT_DECAY, num_epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, multilrstep=DO_MULTILR_STEP_DOWN, gamma=GAMMA)\n",
        "if DUMP_FINAL_RESULTS_ON_GSPREADSHEET:\n",
        "  dump_on_gspreadsheet(CURRENT_RUN, GITHUB_USER, github_link, METHOD, RANDOM_STATE, DO_JOINT_TRAINING, USE_BCE_LOSS, CIFAR_NORMALIZE, group_losses_train, group_losses_eval, group_accuracies_train, group_accuracies_eval, group_accuracies_eval_curr, DURATION, use_validation=USE_VALIDATION_SET, hyperparameters=hyperparameters_string)\n",
        "\n",
        "# audio signal END\n",
        "beep()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "******************************\n",
            "NEW GROUP OF CLASSES 2¬∞/10\n",
            "Training set length: 10000\n",
            "Test set length: 2000\n",
            "Starting epoch 1/70, LR = [2]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:396: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "--- Initial loss on train: 0.7790983319282532\n",
            "--- Epoch 1, Loss on train: 0.03287137299776077\n",
            "Starting epoch 2/70, LR = [2]\n",
            "--- Epoch 2, Loss on train: 0.02829813025891781\n",
            "Starting epoch 3/70, LR = [2]\n",
            "--- Epoch 3, Loss on train: 0.02677612192928791\n",
            "Starting epoch 4/70, LR = [2]\n",
            "--- Epoch 4, Loss on train: 0.025996526703238487\n",
            "Starting epoch 5/70, LR = [2]\n",
            "--- Epoch 5, Loss on train: 0.024525484070181847\n",
            "Starting epoch 6/70, LR = [2]\n",
            "--- Epoch 6, Loss on train: 0.02160685881972313\n",
            "Starting epoch 7/70, LR = [2]\n",
            "--- Epoch 7, Loss on train: 0.022990262135863304\n",
            "Starting epoch 8/70, LR = [2]\n",
            "--- Epoch 8, Loss on train: 0.023973576724529266\n",
            "Starting epoch 9/70, LR = [2]\n",
            "--- Epoch 9, Loss on train: 0.02121874690055847\n",
            "Starting epoch 10/70, LR = [2]\n",
            "--- Epoch 10, Loss on train: 0.02129797451198101\n",
            "Starting epoch 11/70, LR = [2]\n",
            "--- Epoch 11, Loss on train: 0.01933661848306656\n",
            "Starting epoch 12/70, LR = [2]\n",
            "--- Epoch 12, Loss on train: 0.017533449456095695\n",
            "Starting epoch 13/70, LR = [2]\n",
            "--- Epoch 13, Loss on train: 0.017061768099665642\n",
            "Starting epoch 14/70, LR = [2]\n",
            "--- Epoch 14, Loss on train: 0.01722181774675846\n",
            "Starting epoch 15/70, LR = [2]\n",
            "--- Epoch 15, Loss on train: 0.016990309581160545\n",
            "Starting epoch 16/70, LR = [2]\n",
            "--- Epoch 16, Loss on train: 0.015056605450809002\n",
            "Starting epoch 17/70, LR = [2]\n",
            "--- Epoch 17, Loss on train: 0.01463799923658371\n",
            "Starting epoch 18/70, LR = [2]\n",
            "--- Epoch 18, Loss on train: 0.013171304948627949\n",
            "Starting epoch 19/70, LR = [2]\n",
            "--- Epoch 19, Loss on train: 0.011987435631453991\n",
            "Starting epoch 20/70, LR = [2]\n",
            "--- Epoch 20, Loss on train: 0.009995626285672188\n",
            "Starting epoch 21/70, LR = [2]\n",
            "--- Epoch 21, Loss on train: 0.012459799647331238\n",
            "Starting epoch 22/70, LR = [2]\n",
            "--- Epoch 22, Loss on train: 0.011294752359390259\n",
            "Starting epoch 23/70, LR = [2]\n",
            "--- Epoch 23, Loss on train: 0.013247272931039333\n",
            "Starting epoch 24/70, LR = [2]\n",
            "--- Epoch 24, Loss on train: 0.010258384980261326\n",
            "Starting epoch 25/70, LR = [2]\n",
            "--- Epoch 25, Loss on train: 0.009965081699192524\n",
            "Starting epoch 26/70, LR = [2]\n",
            "--- Epoch 26, Loss on train: 0.012668399140238762\n",
            "Starting epoch 27/70, LR = [2]\n",
            "--- Epoch 27, Loss on train: 0.010594133287668228\n",
            "Starting epoch 28/70, LR = [2]\n",
            "--- Epoch 28, Loss on train: 0.007870553992688656\n",
            "Starting epoch 29/70, LR = [2]\n",
            "--- Epoch 29, Loss on train: 0.009199827909469604\n",
            "Starting epoch 30/70, LR = [2]\n",
            "--- Epoch 30, Loss on train: 0.00865558534860611\n",
            "Starting epoch 31/70, LR = [2]\n",
            "--- Epoch 31, Loss on train: 0.010505749844014645\n",
            "Starting epoch 32/70, LR = [2]\n",
            "--- Epoch 32, Loss on train: 0.008846914395689964\n",
            "Starting epoch 33/70, LR = [2]\n",
            "--- Epoch 33, Loss on train: 0.007860210724174976\n",
            "Starting epoch 34/70, LR = [2]\n",
            "--- Epoch 34, Loss on train: 0.010031784884631634\n",
            "Starting epoch 35/70, LR = [2]\n",
            "--- Epoch 35, Loss on train: 0.008464588783681393\n",
            "Starting epoch 36/70, LR = [2]\n",
            "--- Epoch 36, Loss on train: 0.007537714205682278\n",
            "Starting epoch 37/70, LR = [2]\n",
            "--- Epoch 37, Loss on train: 0.009383151307702065\n",
            "Starting epoch 38/70, LR = [2]\n",
            "--- Epoch 38, Loss on train: 0.009572120383381844\n",
            "Starting epoch 39/70, LR = [2]\n",
            "--- Epoch 39, Loss on train: 0.008667600341141224\n",
            "Starting epoch 40/70, LR = [2]\n",
            "--- Epoch 40, Loss on train: 0.008608194068074226\n",
            "Starting epoch 41/70, LR = [2]\n",
            "--- Epoch 41, Loss on train: 0.006921402644366026\n",
            "Starting epoch 42/70, LR = [2]\n",
            "--- Epoch 42, Loss on train: 0.006595614366233349\n",
            "Starting epoch 43/70, LR = [2]\n",
            "--- Epoch 43, Loss on train: 0.007743301335722208\n",
            "Starting epoch 44/70, LR = [2]\n",
            "--- Epoch 44, Loss on train: 0.006413426250219345\n",
            "Starting epoch 45/70, LR = [2]\n",
            "--- Epoch 45, Loss on train: 0.006279538851231337\n",
            "Starting epoch 46/70, LR = [2]\n",
            "--- Epoch 46, Loss on train: 0.005988270044326782\n",
            "Starting epoch 47/70, LR = [2]\n",
            "--- Epoch 47, Loss on train: 0.007314629852771759\n",
            "Starting epoch 48/70, LR = [2]\n",
            "--- Epoch 48, Loss on train: 0.007692018523812294\n",
            "Starting epoch 49/70, LR = [2]\n",
            "--- Epoch 49, Loss on train: 0.008198986761271954\n",
            "Starting epoch 50/70, LR = [0.08000000000000002]\n",
            "--- Epoch 50, Loss on train: 0.004431646782904863\n",
            "Starting epoch 51/70, LR = [0.4]\n",
            "--- Epoch 51, Loss on train: 0.0035702399909496307\n",
            "Starting epoch 52/70, LR = [0.4]\n",
            "--- Epoch 52, Loss on train: 0.004514497239142656\n",
            "Starting epoch 53/70, LR = [0.4]\n",
            "--- Epoch 53, Loss on train: 0.0021543288603425026\n",
            "Starting epoch 54/70, LR = [0.4]\n",
            "--- Epoch 54, Loss on train: 0.0034142977092415094\n",
            "Starting epoch 55/70, LR = [0.4]\n",
            "--- Epoch 55, Loss on train: 0.0028858205769211054\n",
            "Starting epoch 56/70, LR = [0.4]\n",
            "--- Epoch 56, Loss on train: 0.00310394074767828\n",
            "Starting epoch 57/70, LR = [0.4]\n",
            "--- Epoch 57, Loss on train: 0.0024349207524210215\n",
            "Starting epoch 58/70, LR = [0.4]\n",
            "--- Epoch 58, Loss on train: 0.002434884663671255\n",
            "Starting epoch 59/70, LR = [0.4]\n",
            "--- Epoch 59, Loss on train: 0.0014314398868009448\n",
            "Starting epoch 60/70, LR = [0.4]\n",
            "--- Epoch 60, Loss on train: 0.0023819999769330025\n",
            "Starting epoch 61/70, LR = [0.4]\n",
            "--- Epoch 61, Loss on train: 0.002396433614194393\n",
            "Starting epoch 62/70, LR = [0.4]\n",
            "--- Epoch 62, Loss on train: 0.002384057268500328\n",
            "Starting epoch 63/70, LR = [0.4]\n",
            "--- Epoch 63, Loss on train: 0.003269190201535821\n",
            "Starting epoch 64/70, LR = [0.016000000000000004]\n",
            "--- Epoch 64, Loss on train: 0.0017052131006494164\n",
            "Starting epoch 65/70, LR = [0.08000000000000002]\n",
            "--- Epoch 65, Loss on train: 0.0013630288885906339\n",
            "Starting epoch 66/70, LR = [0.08000000000000002]\n",
            "--- Epoch 66, Loss on train: 0.0012538837036117911\n",
            "Starting epoch 67/70, LR = [0.08000000000000002]\n",
            "--- Epoch 67, Loss on train: 0.0018015861278399825\n",
            "Starting epoch 68/70, LR = [0.08000000000000002]\n",
            "--- Epoch 68, Loss on train: 0.0011015144409611821\n",
            "Starting epoch 69/70, LR = [0.08000000000000002]\n",
            "--- Epoch 69, Loss on train: 0.00122439197730273\n",
            "Starting epoch 70/70, LR = [0.08000000000000002]\n",
            "--- Epoch 70, Loss on train: 0.001636593951843679\n",
            "Loss on eval (group): 0.00043316190876066687\n",
            "Accuracy on eval (group): 0.837\n",
            "Accuracy on train (group): 0.9882\n",
            "******************************\n",
            "NEW GROUP OF CLASSES 3¬∞/10\n",
            "Training set length: 15000\n",
            "Test set length: 3000\n",
            "Starting epoch 1/70, LR = [2]\n",
            "--- Initial loss on train: 0.7799072265625\n",
            "--- Epoch 1, Loss on train: 0.039761416614055634\n",
            "Starting epoch 2/70, LR = [2]\n",
            "--- Epoch 2, Loss on train: 0.03319184109568596\n",
            "Starting epoch 3/70, LR = [2]\n",
            "--- Epoch 3, Loss on train: 0.03425926715135574\n",
            "Starting epoch 4/70, LR = [2]\n",
            "--- Epoch 4, Loss on train: 0.03010542504489422\n",
            "Starting epoch 5/70, LR = [2]\n",
            "--- Epoch 5, Loss on train: 0.02602599561214447\n",
            "Starting epoch 6/70, LR = [2]\n",
            "--- Epoch 6, Loss on train: 0.025987403467297554\n",
            "Starting epoch 7/70, LR = [2]\n",
            "--- Epoch 7, Loss on train: 0.024711906909942627\n",
            "Starting epoch 8/70, LR = [2]\n",
            "--- Epoch 8, Loss on train: 0.024396862834692\n",
            "Starting epoch 9/70, LR = [2]\n",
            "--- Epoch 9, Loss on train: 0.02393840253353119\n",
            "Starting epoch 10/70, LR = [2]\n",
            "--- Epoch 10, Loss on train: 0.02419145219027996\n",
            "Starting epoch 11/70, LR = [2]\n",
            "--- Epoch 11, Loss on train: 0.021954374387860298\n",
            "Starting epoch 12/70, LR = [2]\n",
            "--- Epoch 12, Loss on train: 0.02116984874010086\n",
            "Starting epoch 13/70, LR = [2]\n",
            "--- Epoch 13, Loss on train: 0.020680533722043037\n",
            "Starting epoch 14/70, LR = [2]\n",
            "--- Epoch 14, Loss on train: 0.01847688853740692\n",
            "Starting epoch 15/70, LR = [2]\n",
            "--- Epoch 15, Loss on train: 0.019671635702252388\n",
            "Starting epoch 16/70, LR = [2]\n",
            "--- Epoch 16, Loss on train: 0.017073282971978188\n",
            "Starting epoch 17/70, LR = [2]\n",
            "--- Epoch 17, Loss on train: 0.016822341829538345\n",
            "Starting epoch 18/70, LR = [2]\n",
            "--- Epoch 18, Loss on train: 0.017082998529076576\n",
            "Starting epoch 19/70, LR = [2]\n",
            "--- Epoch 19, Loss on train: 0.017157329246401787\n",
            "Starting epoch 20/70, LR = [2]\n",
            "--- Epoch 20, Loss on train: 0.015032386407256126\n",
            "Starting epoch 21/70, LR = [2]\n",
            "--- Epoch 21, Loss on train: 0.013502771966159344\n",
            "Starting epoch 22/70, LR = [2]\n",
            "--- Epoch 22, Loss on train: 0.01712564378976822\n",
            "Starting epoch 23/70, LR = [2]\n",
            "--- Epoch 23, Loss on train: 0.015381474047899246\n",
            "Starting epoch 24/70, LR = [2]\n",
            "--- Epoch 24, Loss on train: 0.013597443699836731\n",
            "Starting epoch 25/70, LR = [2]\n",
            "--- Epoch 25, Loss on train: 0.013880642130970955\n",
            "Starting epoch 26/70, LR = [2]\n",
            "--- Epoch 26, Loss on train: 0.012332762591540813\n",
            "Starting epoch 27/70, LR = [2]\n",
            "--- Epoch 27, Loss on train: 0.0113821504637599\n",
            "Starting epoch 28/70, LR = [2]\n",
            "--- Epoch 28, Loss on train: 0.013717439025640488\n",
            "Starting epoch 29/70, LR = [2]\n",
            "--- Epoch 29, Loss on train: 0.010569248348474503\n",
            "Starting epoch 30/70, LR = [2]\n",
            "--- Epoch 30, Loss on train: 0.01507168635725975\n",
            "Starting epoch 31/70, LR = [2]\n",
            "--- Epoch 31, Loss on train: 0.011121626012027264\n",
            "Starting epoch 32/70, LR = [2]\n",
            "--- Epoch 32, Loss on train: 0.013577786274254322\n",
            "Starting epoch 33/70, LR = [2]\n",
            "--- Epoch 33, Loss on train: 0.013562011532485485\n",
            "Starting epoch 34/70, LR = [2]\n",
            "--- Epoch 34, Loss on train: 0.009610859677195549\n",
            "Starting epoch 35/70, LR = [2]\n",
            "--- Epoch 35, Loss on train: 0.00951296091079712\n",
            "Starting epoch 36/70, LR = [2]\n",
            "--- Epoch 36, Loss on train: 0.009443596005439758\n",
            "Starting epoch 37/70, LR = [2]\n",
            "--- Epoch 37, Loss on train: 0.011727014556527138\n",
            "Starting epoch 38/70, LR = [2]\n",
            "--- Epoch 38, Loss on train: 0.011610720306634903\n",
            "Starting epoch 39/70, LR = [2]\n",
            "--- Epoch 39, Loss on train: 0.010307669639587402\n",
            "Starting epoch 40/70, LR = [2]\n",
            "--- Epoch 40, Loss on train: 0.00915736798197031\n",
            "Starting epoch 41/70, LR = [2]\n",
            "--- Epoch 41, Loss on train: 0.009830882772803307\n",
            "Starting epoch 42/70, LR = [2]\n",
            "--- Epoch 42, Loss on train: 0.00776204327121377\n",
            "Starting epoch 43/70, LR = [2]\n",
            "--- Epoch 43, Loss on train: 0.011653616093099117\n",
            "Starting epoch 44/70, LR = [2]\n",
            "--- Epoch 44, Loss on train: 0.010572442784905434\n",
            "Starting epoch 45/70, LR = [2]\n",
            "--- Epoch 45, Loss on train: 0.010257364250719547\n",
            "Starting epoch 46/70, LR = [2]\n",
            "--- Epoch 46, Loss on train: 0.010258057154715061\n",
            "Starting epoch 47/70, LR = [2]\n",
            "--- Epoch 47, Loss on train: 0.007274562027305365\n",
            "Starting epoch 48/70, LR = [2]\n",
            "--- Epoch 48, Loss on train: 0.007261999882757664\n",
            "Starting epoch 49/70, LR = [2]\n",
            "--- Epoch 49, Loss on train: 0.00710287457332015\n",
            "Starting epoch 50/70, LR = [0.08000000000000002]\n",
            "--- Epoch 50, Loss on train: 0.005709963850677013\n",
            "Starting epoch 51/70, LR = [0.4]\n",
            "--- Epoch 51, Loss on train: 0.004079620353877544\n",
            "Starting epoch 52/70, LR = [0.4]\n",
            "--- Epoch 52, Loss on train: 0.0037619664799422026\n",
            "Starting epoch 53/70, LR = [0.4]\n",
            "--- Epoch 53, Loss on train: 0.005480439402163029\n",
            "Starting epoch 54/70, LR = [0.4]\n",
            "--- Epoch 54, Loss on train: 0.004546654876321554\n",
            "Starting epoch 55/70, LR = [0.4]\n",
            "--- Epoch 55, Loss on train: 0.00437713460996747\n",
            "Starting epoch 56/70, LR = [0.4]\n",
            "--- Epoch 56, Loss on train: 0.004389062989503145\n",
            "Starting epoch 57/70, LR = [0.4]\n",
            "--- Epoch 57, Loss on train: 0.004550945479422808\n",
            "Starting epoch 58/70, LR = [0.4]\n",
            "--- Epoch 58, Loss on train: 0.003816162468865514\n",
            "Starting epoch 59/70, LR = [0.4]\n",
            "--- Epoch 59, Loss on train: 0.0038325630594044924\n",
            "Starting epoch 60/70, LR = [0.4]\n",
            "--- Epoch 60, Loss on train: 0.003739501815289259\n",
            "Starting epoch 61/70, LR = [0.4]\n",
            "--- Epoch 61, Loss on train: 0.005629471503198147\n",
            "Starting epoch 62/70, LR = [0.4]\n",
            "--- Epoch 62, Loss on train: 0.003590809414163232\n",
            "Starting epoch 63/70, LR = [0.4]\n",
            "--- Epoch 63, Loss on train: 0.002115784212946892\n",
            "Starting epoch 64/70, LR = [0.016000000000000004]\n",
            "--- Epoch 64, Loss on train: 0.0027409319300204515\n",
            "Starting epoch 65/70, LR = [0.08000000000000002]\n",
            "--- Epoch 65, Loss on train: 0.0020538808312267065\n",
            "Starting epoch 66/70, LR = [0.08000000000000002]\n",
            "--- Epoch 66, Loss on train: 0.003310535801574588\n",
            "Starting epoch 67/70, LR = [0.08000000000000002]\n",
            "--- Epoch 67, Loss on train: 0.0028894920833408833\n",
            "Starting epoch 68/70, LR = [0.08000000000000002]\n",
            "--- Epoch 68, Loss on train: 0.002190336352214217\n",
            "Starting epoch 69/70, LR = [0.08000000000000002]\n",
            "--- Epoch 69, Loss on train: 0.0018776338547468185\n",
            "Starting epoch 70/70, LR = [0.08000000000000002]\n",
            "--- Epoch 70, Loss on train: 0.002538174856454134\n",
            "Loss on eval (group): 0.00032987648559113344\n",
            "Accuracy on eval (group): 0.8076666666666666\n",
            "Accuracy on train (group): 0.9808666666666667\n",
            "******************************\n",
            "NEW GROUP OF CLASSES 4¬∞/10\n",
            "Training set length: 20000\n",
            "Test set length: 4000\n",
            "Starting epoch 1/70, LR = [2]\n",
            "--- Initial loss on train: 0.7554415464401245\n",
            "--- Epoch 1, Loss on train: 0.03931265324354172\n",
            "Starting epoch 2/70, LR = [2]\n",
            "--- Epoch 2, Loss on train: 0.03697655722498894\n",
            "Starting epoch 3/70, LR = [2]\n",
            "--- Epoch 3, Loss on train: 0.0361030250787735\n",
            "Starting epoch 4/70, LR = [2]\n",
            "--- Epoch 4, Loss on train: 0.03385866433382034\n",
            "Starting epoch 5/70, LR = [2]\n",
            "--- Epoch 5, Loss on train: 0.030839314684271812\n",
            "Starting epoch 6/70, LR = [2]\n",
            "--- Epoch 6, Loss on train: 0.02951223775744438\n",
            "Starting epoch 7/70, LR = [2]\n",
            "--- Epoch 7, Loss on train: 0.02641880512237549\n",
            "Starting epoch 8/70, LR = [2]\n",
            "--- Epoch 8, Loss on train: 0.026618415489792824\n",
            "Starting epoch 9/70, LR = [2]\n",
            "--- Epoch 9, Loss on train: 0.025108372792601585\n",
            "Starting epoch 10/70, LR = [2]\n",
            "--- Epoch 10, Loss on train: 0.024417204782366753\n",
            "Starting epoch 11/70, LR = [2]\n",
            "--- Epoch 11, Loss on train: 0.02382368966937065\n",
            "Starting epoch 12/70, LR = [2]\n",
            "--- Epoch 12, Loss on train: 0.021137012168765068\n",
            "Starting epoch 13/70, LR = [2]\n",
            "--- Epoch 13, Loss on train: 0.020891224965453148\n",
            "Starting epoch 14/70, LR = [2]\n",
            "--- Epoch 14, Loss on train: 0.020312031731009483\n",
            "Starting epoch 15/70, LR = [2]\n",
            "--- Epoch 15, Loss on train: 0.018807822838425636\n",
            "Starting epoch 16/70, LR = [2]\n",
            "--- Epoch 16, Loss on train: 0.01728690415620804\n",
            "Starting epoch 17/70, LR = [2]\n",
            "--- Epoch 17, Loss on train: 0.01732213981449604\n",
            "Starting epoch 18/70, LR = [2]\n",
            "--- Epoch 18, Loss on train: 0.017992490902543068\n",
            "Starting epoch 19/70, LR = [2]\n",
            "--- Epoch 19, Loss on train: 0.01580079086124897\n",
            "Starting epoch 20/70, LR = [2]\n",
            "--- Epoch 20, Loss on train: 0.01978013478219509\n",
            "Starting epoch 21/70, LR = [2]\n",
            "--- Epoch 21, Loss on train: 0.016052717342972755\n",
            "Starting epoch 22/70, LR = [2]\n",
            "--- Epoch 22, Loss on train: 0.01647004298865795\n",
            "Starting epoch 23/70, LR = [2]\n",
            "--- Epoch 23, Loss on train: 0.013765322044491768\n",
            "Starting epoch 24/70, LR = [2]\n",
            "--- Epoch 24, Loss on train: 0.017026564106345177\n",
            "Starting epoch 25/70, LR = [2]\n",
            "--- Epoch 25, Loss on train: 0.015879418700933456\n",
            "Starting epoch 26/70, LR = [2]\n",
            "--- Epoch 26, Loss on train: 0.01528322882950306\n",
            "Starting epoch 27/70, LR = [2]\n",
            "--- Epoch 27, Loss on train: 0.012459741905331612\n",
            "Starting epoch 28/70, LR = [2]\n",
            "--- Epoch 28, Loss on train: 0.013537676073610783\n",
            "Starting epoch 29/70, LR = [2]\n",
            "--- Epoch 29, Loss on train: 0.014599816873669624\n",
            "Starting epoch 30/70, LR = [2]\n",
            "--- Epoch 30, Loss on train: 0.013365838676691055\n",
            "Starting epoch 31/70, LR = [2]\n",
            "--- Epoch 31, Loss on train: 0.015090808272361755\n",
            "Starting epoch 32/70, LR = [2]\n",
            "--- Epoch 32, Loss on train: 0.012224290519952774\n",
            "Starting epoch 33/70, LR = [2]\n",
            "--- Epoch 33, Loss on train: 0.014193524606525898\n",
            "Starting epoch 34/70, LR = [2]\n",
            "--- Epoch 34, Loss on train: 0.015354523435235023\n",
            "Starting epoch 35/70, LR = [2]\n",
            "--- Epoch 35, Loss on train: 0.01561689842492342\n",
            "Starting epoch 36/70, LR = [2]\n",
            "--- Epoch 36, Loss on train: 0.012245017103850842\n",
            "Starting epoch 37/70, LR = [2]\n",
            "--- Epoch 37, Loss on train: 0.011459882371127605\n",
            "Starting epoch 38/70, LR = [2]\n",
            "--- Epoch 38, Loss on train: 0.013926872983574867\n",
            "Starting epoch 39/70, LR = [2]\n",
            "--- Epoch 39, Loss on train: 0.013013627380132675\n",
            "Starting epoch 40/70, LR = [2]\n",
            "--- Epoch 40, Loss on train: 0.012986921705305576\n",
            "Starting epoch 41/70, LR = [2]\n",
            "--- Epoch 41, Loss on train: 0.011741227470338345\n",
            "Starting epoch 42/70, LR = [2]\n",
            "--- Epoch 42, Loss on train: 0.011470869183540344\n",
            "Starting epoch 43/70, LR = [2]\n",
            "--- Epoch 43, Loss on train: 0.011855080723762512\n",
            "Starting epoch 44/70, LR = [2]\n",
            "--- Epoch 44, Loss on train: 0.01384757924824953\n",
            "Starting epoch 45/70, LR = [2]\n",
            "--- Epoch 45, Loss on train: 0.01127469539642334\n",
            "Starting epoch 46/70, LR = [2]\n",
            "--- Epoch 46, Loss on train: 0.010482890531420708\n",
            "Starting epoch 47/70, LR = [2]\n",
            "--- Epoch 47, Loss on train: 0.011316265910863876\n",
            "Starting epoch 48/70, LR = [2]\n",
            "--- Epoch 48, Loss on train: 0.009254132397472858\n",
            "Starting epoch 49/70, LR = [2]\n",
            "--- Epoch 49, Loss on train: 0.010273270308971405\n",
            "Starting epoch 50/70, LR = [0.08000000000000002]\n",
            "--- Epoch 50, Loss on train: 0.009184259921312332\n",
            "Starting epoch 51/70, LR = [0.4]\n",
            "--- Epoch 51, Loss on train: 0.0069875335320830345\n",
            "Starting epoch 52/70, LR = [0.4]\n",
            "--- Epoch 52, Loss on train: 0.005651893559843302\n",
            "Starting epoch 53/70, LR = [0.4]\n",
            "--- Epoch 53, Loss on train: 0.006011458579450846\n",
            "Starting epoch 54/70, LR = [0.4]\n",
            "--- Epoch 54, Loss on train: 0.005570541601628065\n",
            "Starting epoch 55/70, LR = [0.4]\n",
            "--- Epoch 55, Loss on train: 0.008326871320605278\n",
            "Starting epoch 56/70, LR = [0.4]\n",
            "--- Epoch 56, Loss on train: 0.004827911965548992\n",
            "Starting epoch 57/70, LR = [0.4]\n",
            "--- Epoch 57, Loss on train: 0.005085112992674112\n",
            "Starting epoch 58/70, LR = [0.4]\n",
            "--- Epoch 58, Loss on train: 0.005150717217475176\n",
            "Starting epoch 59/70, LR = [0.4]\n",
            "--- Epoch 59, Loss on train: 0.0049532148987054825\n",
            "Starting epoch 60/70, LR = [0.4]\n",
            "--- Epoch 60, Loss on train: 0.004692989867180586\n",
            "Starting epoch 61/70, LR = [0.4]\n",
            "--- Epoch 61, Loss on train: 0.004267458338290453\n",
            "Starting epoch 62/70, LR = [0.4]\n",
            "--- Epoch 62, Loss on train: 0.004836993291974068\n",
            "Starting epoch 63/70, LR = [0.4]\n",
            "--- Epoch 63, Loss on train: 0.0060249995440244675\n",
            "Starting epoch 64/70, LR = [0.016000000000000004]\n",
            "--- Epoch 64, Loss on train: 0.00402092793956399\n",
            "Starting epoch 65/70, LR = [0.08000000000000002]\n",
            "--- Epoch 65, Loss on train: 0.003056506859138608\n",
            "Starting epoch 66/70, LR = [0.08000000000000002]\n",
            "--- Epoch 66, Loss on train: 0.003401843598112464\n",
            "Starting epoch 67/70, LR = [0.08000000000000002]\n",
            "--- Epoch 67, Loss on train: 0.003575787413865328\n",
            "Starting epoch 68/70, LR = [0.08000000000000002]\n",
            "--- Epoch 68, Loss on train: 0.00350728421472013\n",
            "Starting epoch 69/70, LR = [0.08000000000000002]\n",
            "--- Epoch 69, Loss on train: 0.003683044807985425\n",
            "Starting epoch 70/70, LR = [0.08000000000000002]\n",
            "--- Epoch 70, Loss on train: 0.0014899407979100943\n",
            "Loss on eval (group): 0.000266130052972585\n",
            "Accuracy on eval (group): 0.77975\n",
            "Accuracy on train (group): 0.96345\n",
            "******************************\n",
            "NEW GROUP OF CLASSES 5¬∞/10\n",
            "Training set length: 25000\n",
            "Test set length: 5000\n",
            "Starting epoch 1/70, LR = [2]\n",
            "--- Initial loss on train: 0.7896246910095215\n",
            "--- Epoch 1, Loss on train: 0.04358846694231033\n",
            "Starting epoch 2/70, LR = [2]\n",
            "--- Epoch 2, Loss on train: 0.040990378707647324\n",
            "Starting epoch 3/70, LR = [2]\n",
            "--- Epoch 3, Loss on train: 0.035978302359580994\n",
            "Starting epoch 4/70, LR = [2]\n",
            "--- Epoch 4, Loss on train: 0.035107459872961044\n",
            "Starting epoch 5/70, LR = [2]\n",
            "--- Epoch 5, Loss on train: 0.035369209945201874\n",
            "Starting epoch 6/70, LR = [2]\n",
            "--- Epoch 6, Loss on train: 0.03357908874750137\n",
            "Starting epoch 7/70, LR = [2]\n",
            "--- Epoch 7, Loss on train: 0.030155031010508537\n",
            "Starting epoch 8/70, LR = [2]\n",
            "--- Epoch 8, Loss on train: 0.0272524431347847\n",
            "Starting epoch 9/70, LR = [2]\n",
            "--- Epoch 9, Loss on train: 0.02355707436800003\n",
            "Starting epoch 10/70, LR = [2]\n",
            "--- Epoch 10, Loss on train: 0.026269664987921715\n",
            "Starting epoch 11/70, LR = [2]\n",
            "--- Epoch 11, Loss on train: 0.024524355307221413\n",
            "Starting epoch 12/70, LR = [2]\n",
            "--- Epoch 12, Loss on train: 0.025263676419854164\n",
            "Starting epoch 13/70, LR = [2]\n",
            "--- Epoch 13, Loss on train: 0.022420521825551987\n",
            "Starting epoch 14/70, LR = [2]\n",
            "--- Epoch 14, Loss on train: 0.025692882016301155\n",
            "Starting epoch 15/70, LR = [2]\n",
            "--- Epoch 15, Loss on train: 0.021461956202983856\n",
            "Starting epoch 16/70, LR = [2]\n",
            "--- Epoch 16, Loss on train: 0.021532485261559486\n",
            "Starting epoch 17/70, LR = [2]\n",
            "--- Epoch 17, Loss on train: 0.021134328097105026\n",
            "Starting epoch 18/70, LR = [2]\n",
            "--- Epoch 18, Loss on train: 0.02049589902162552\n",
            "Starting epoch 19/70, LR = [2]\n",
            "--- Epoch 19, Loss on train: 0.019679469987750053\n",
            "Starting epoch 20/70, LR = [2]\n",
            "--- Epoch 20, Loss on train: 0.018791725859045982\n",
            "Starting epoch 21/70, LR = [2]\n",
            "--- Epoch 21, Loss on train: 0.017398230731487274\n",
            "Starting epoch 22/70, LR = [2]\n",
            "--- Epoch 22, Loss on train: 0.01807519420981407\n",
            "Starting epoch 23/70, LR = [2]\n",
            "--- Epoch 23, Loss on train: 0.019414879381656647\n",
            "Starting epoch 24/70, LR = [2]\n",
            "--- Epoch 24, Loss on train: 0.01451296266168356\n",
            "Starting epoch 25/70, LR = [2]\n",
            "--- Epoch 25, Loss on train: 0.018290193751454353\n",
            "Starting epoch 26/70, LR = [2]\n",
            "--- Epoch 26, Loss on train: 0.01704067923128605\n",
            "Starting epoch 27/70, LR = [2]\n",
            "--- Epoch 27, Loss on train: 0.018212925642728806\n",
            "Starting epoch 28/70, LR = [2]\n",
            "--- Epoch 28, Loss on train: 0.017761968076229095\n",
            "Starting epoch 29/70, LR = [2]\n",
            "--- Epoch 29, Loss on train: 0.01628425344824791\n",
            "Starting epoch 30/70, LR = [2]\n",
            "--- Epoch 30, Loss on train: 0.014733039774000645\n",
            "Starting epoch 31/70, LR = [2]\n",
            "--- Epoch 31, Loss on train: 0.014382007531821728\n",
            "Starting epoch 32/70, LR = [2]\n",
            "--- Epoch 32, Loss on train: 0.015169539488852024\n",
            "Starting epoch 33/70, LR = [2]\n",
            "--- Epoch 33, Loss on train: 0.015201805159449577\n",
            "Starting epoch 34/70, LR = [2]\n",
            "--- Epoch 34, Loss on train: 0.015224787406623363\n",
            "Starting epoch 35/70, LR = [2]\n",
            "--- Epoch 35, Loss on train: 0.013904516585171223\n",
            "Starting epoch 36/70, LR = [2]\n",
            "--- Epoch 36, Loss on train: 0.014905538409948349\n",
            "Starting epoch 37/70, LR = [2]\n",
            "--- Epoch 37, Loss on train: 0.01732444576919079\n",
            "Starting epoch 38/70, LR = [2]\n",
            "--- Epoch 38, Loss on train: 0.01371671911329031\n",
            "Starting epoch 39/70, LR = [2]\n",
            "--- Epoch 39, Loss on train: 0.011869046837091446\n",
            "Starting epoch 40/70, LR = [2]\n",
            "--- Epoch 40, Loss on train: 0.014656825922429562\n",
            "Starting epoch 41/70, LR = [2]\n",
            "--- Epoch 41, Loss on train: 0.012765396386384964\n",
            "Starting epoch 42/70, LR = [2]\n",
            "--- Epoch 42, Loss on train: 0.014965887181460857\n",
            "Starting epoch 43/70, LR = [2]\n",
            "--- Epoch 43, Loss on train: 0.01433516200631857\n",
            "Starting epoch 44/70, LR = [2]\n",
            "--- Epoch 44, Loss on train: 0.016515647992491722\n",
            "Starting epoch 45/70, LR = [2]\n",
            "--- Epoch 45, Loss on train: 0.014722032472491264\n",
            "Starting epoch 46/70, LR = [2]\n",
            "--- Epoch 46, Loss on train: 0.013788542710244656\n",
            "Starting epoch 47/70, LR = [2]\n",
            "--- Epoch 47, Loss on train: 0.012564058415591717\n",
            "Starting epoch 48/70, LR = [2]\n",
            "--- Epoch 48, Loss on train: 0.013366352766752243\n",
            "Starting epoch 49/70, LR = [2]\n",
            "--- Epoch 49, Loss on train: 0.012832245789468288\n",
            "Starting epoch 50/70, LR = [0.08000000000000002]\n",
            "--- Epoch 50, Loss on train: 0.01018171850591898\n",
            "Starting epoch 51/70, LR = [0.4]\n",
            "--- Epoch 51, Loss on train: 0.01053120568394661\n",
            "Starting epoch 52/70, LR = [0.4]\n",
            "--- Epoch 52, Loss on train: 0.009225231595337391\n",
            "Starting epoch 53/70, LR = [0.4]\n",
            "--- Epoch 53, Loss on train: 0.007702781818807125\n",
            "Starting epoch 54/70, LR = [0.4]\n",
            "--- Epoch 54, Loss on train: 0.005845923442393541\n",
            "Starting epoch 55/70, LR = [0.4]\n",
            "--- Epoch 55, Loss on train: 0.007593334652483463\n",
            "Starting epoch 56/70, LR = [0.4]\n",
            "--- Epoch 56, Loss on train: 0.008548635989427567\n",
            "Starting epoch 57/70, LR = [0.4]\n",
            "--- Epoch 57, Loss on train: 0.01133057288825512\n",
            "Starting epoch 58/70, LR = [0.4]\n",
            "--- Epoch 58, Loss on train: 0.007372922729700804\n",
            "Starting epoch 59/70, LR = [0.4]\n",
            "--- Epoch 59, Loss on train: 0.006043820176273584\n",
            "Starting epoch 60/70, LR = [0.4]\n",
            "--- Epoch 60, Loss on train: 0.007502404507249594\n",
            "Starting epoch 61/70, LR = [0.4]\n",
            "--- Epoch 61, Loss on train: 0.005456665065139532\n",
            "Starting epoch 62/70, LR = [0.4]\n",
            "--- Epoch 62, Loss on train: 0.007186186034232378\n",
            "Starting epoch 63/70, LR = [0.4]\n",
            "--- Epoch 63, Loss on train: 0.00478016585111618\n",
            "Starting epoch 64/70, LR = [0.016000000000000004]\n",
            "--- Epoch 64, Loss on train: 0.004990010522305965\n",
            "Starting epoch 65/70, LR = [0.08000000000000002]\n",
            "--- Epoch 65, Loss on train: 0.00443667359650135\n",
            "Starting epoch 66/70, LR = [0.08000000000000002]\n",
            "--- Epoch 66, Loss on train: 0.0056001790799200535\n",
            "Starting epoch 67/70, LR = [0.08000000000000002]\n",
            "--- Epoch 67, Loss on train: 0.004916868172585964\n",
            "Starting epoch 68/70, LR = [0.08000000000000002]\n",
            "--- Epoch 68, Loss on train: 0.004392033908516169\n",
            "Starting epoch 69/70, LR = [0.08000000000000002]\n",
            "--- Epoch 69, Loss on train: 0.0055003697052598\n",
            "Starting epoch 70/70, LR = [0.08000000000000002]\n",
            "--- Epoch 70, Loss on train: 0.0050122798420488834\n",
            "Loss on eval (group): 0.00022635404691100121\n",
            "Accuracy on eval (group): 0.7568\n",
            "Accuracy on train (group): 0.9438\n",
            "******************************\n",
            "NEW GROUP OF CLASSES 6¬∞/10\n",
            "Training set length: 30000\n",
            "Test set length: 6000\n",
            "Starting epoch 1/70, LR = [2]\n",
            "--- Initial loss on train: 0.7740659117698669\n",
            "--- Epoch 1, Loss on train: 0.045281074941158295\n",
            "Starting epoch 2/70, LR = [2]\n",
            "--- Epoch 2, Loss on train: 0.044313669204711914\n",
            "Starting epoch 3/70, LR = [2]\n",
            "--- Epoch 3, Loss on train: 0.03983572497963905\n",
            "Starting epoch 4/70, LR = [2]\n",
            "--- Epoch 4, Loss on train: 0.03884468600153923\n",
            "Starting epoch 5/70, LR = [2]\n",
            "--- Epoch 5, Loss on train: 0.03567613661289215\n",
            "Starting epoch 6/70, LR = [2]\n",
            "--- Epoch 6, Loss on train: 0.03394865617156029\n",
            "Starting epoch 7/70, LR = [2]\n",
            "--- Epoch 7, Loss on train: 0.03536084294319153\n",
            "Starting epoch 8/70, LR = [2]\n",
            "--- Epoch 8, Loss on train: 0.031085092574357986\n",
            "Starting epoch 9/70, LR = [2]\n",
            "--- Epoch 9, Loss on train: 0.02931426465511322\n",
            "Starting epoch 10/70, LR = [2]\n",
            "--- Epoch 10, Loss on train: 0.03192692622542381\n",
            "Starting epoch 11/70, LR = [2]\n",
            "--- Epoch 11, Loss on train: 0.026537630707025528\n",
            "Starting epoch 12/70, LR = [2]\n",
            "--- Epoch 12, Loss on train: 0.025980131700634956\n",
            "Starting epoch 13/70, LR = [2]\n",
            "--- Epoch 13, Loss on train: 0.026625731959939003\n",
            "Starting epoch 14/70, LR = [2]\n",
            "--- Epoch 14, Loss on train: 0.026217445731163025\n",
            "Starting epoch 15/70, LR = [2]\n",
            "--- Epoch 15, Loss on train: 0.0215747132897377\n",
            "Starting epoch 16/70, LR = [2]\n",
            "--- Epoch 16, Loss on train: 0.022323288023471832\n",
            "Starting epoch 17/70, LR = [2]\n",
            "--- Epoch 17, Loss on train: 0.02105870470404625\n",
            "Starting epoch 18/70, LR = [2]\n",
            "--- Epoch 18, Loss on train: 0.021180452778935432\n",
            "Starting epoch 19/70, LR = [2]\n",
            "--- Epoch 19, Loss on train: 0.022033104673027992\n",
            "Starting epoch 20/70, LR = [2]\n",
            "--- Epoch 20, Loss on train: 0.01997009664773941\n",
            "Starting epoch 21/70, LR = [2]\n",
            "--- Epoch 21, Loss on train: 0.01955966278910637\n",
            "Starting epoch 22/70, LR = [2]\n",
            "--- Epoch 22, Loss on train: 0.02154877595603466\n",
            "Starting epoch 23/70, LR = [2]\n",
            "--- Epoch 23, Loss on train: 0.01803545653820038\n",
            "Starting epoch 24/70, LR = [2]\n",
            "--- Epoch 24, Loss on train: 0.018582219257950783\n",
            "Starting epoch 25/70, LR = [2]\n",
            "--- Epoch 25, Loss on train: 0.020063871517777443\n",
            "Starting epoch 26/70, LR = [2]\n",
            "--- Epoch 26, Loss on train: 0.019143488258123398\n",
            "Starting epoch 27/70, LR = [2]\n",
            "--- Epoch 27, Loss on train: 0.019418954849243164\n",
            "Starting epoch 28/70, LR = [2]\n",
            "--- Epoch 28, Loss on train: 0.017908897250890732\n",
            "Starting epoch 29/70, LR = [2]\n",
            "--- Epoch 29, Loss on train: 0.015940168872475624\n",
            "Starting epoch 30/70, LR = [2]\n",
            "--- Epoch 30, Loss on train: 0.016716063022613525\n",
            "Starting epoch 31/70, LR = [2]\n",
            "--- Epoch 31, Loss on train: 0.01740017719566822\n",
            "Starting epoch 32/70, LR = [2]\n",
            "--- Epoch 32, Loss on train: 0.018669098615646362\n",
            "Starting epoch 33/70, LR = [2]\n",
            "--- Epoch 33, Loss on train: 0.016530487686395645\n",
            "Starting epoch 34/70, LR = [2]\n",
            "--- Epoch 34, Loss on train: 0.015193471685051918\n",
            "Starting epoch 35/70, LR = [2]\n",
            "--- Epoch 35, Loss on train: 0.016590852290391922\n",
            "Starting epoch 36/70, LR = [2]\n",
            "--- Epoch 36, Loss on train: 0.01723732054233551\n",
            "Starting epoch 37/70, LR = [2]\n",
            "--- Epoch 37, Loss on train: 0.016208350658416748\n",
            "Starting epoch 38/70, LR = [2]\n",
            "--- Epoch 38, Loss on train: 0.013071336783468723\n",
            "Starting epoch 39/70, LR = [2]\n",
            "--- Epoch 39, Loss on train: 0.017982086166739464\n",
            "Starting epoch 40/70, LR = [2]\n",
            "--- Epoch 40, Loss on train: 0.017054326832294464\n",
            "Starting epoch 41/70, LR = [2]\n",
            "--- Epoch 41, Loss on train: 0.0162601750344038\n",
            "Starting epoch 42/70, LR = [2]\n",
            "--- Epoch 42, Loss on train: 0.01687592640519142\n",
            "Starting epoch 43/70, LR = [2]\n",
            "--- Epoch 43, Loss on train: 0.014416968449950218\n",
            "Starting epoch 44/70, LR = [2]\n",
            "--- Epoch 44, Loss on train: 0.014174453914165497\n",
            "Starting epoch 45/70, LR = [2]\n",
            "--- Epoch 45, Loss on train: 0.014017722569406033\n",
            "Starting epoch 46/70, LR = [2]\n",
            "--- Epoch 46, Loss on train: 0.01365995779633522\n",
            "Starting epoch 47/70, LR = [2]\n",
            "--- Epoch 47, Loss on train: 0.014831622131168842\n",
            "Starting epoch 48/70, LR = [2]\n",
            "--- Epoch 48, Loss on train: 0.014447755180299282\n",
            "Starting epoch 49/70, LR = [2]\n",
            "--- Epoch 49, Loss on train: 0.014788740314543247\n",
            "Starting epoch 50/70, LR = [0.08000000000000002]\n",
            "--- Epoch 50, Loss on train: 0.010193996131420135\n",
            "Starting epoch 51/70, LR = [0.4]\n",
            "--- Epoch 51, Loss on train: 0.007578869815915823\n",
            "Starting epoch 52/70, LR = [0.4]\n",
            "--- Epoch 52, Loss on train: 0.008517037145793438\n",
            "Starting epoch 53/70, LR = [0.4]\n",
            "--- Epoch 53, Loss on train: 0.00851750560104847\n",
            "Starting epoch 54/70, LR = [0.4]\n",
            "--- Epoch 54, Loss on train: 0.007722761482000351\n",
            "Starting epoch 55/70, LR = [0.4]\n",
            "--- Epoch 55, Loss on train: 0.0081245768815279\n",
            "Starting epoch 56/70, LR = [0.4]\n",
            "--- Epoch 56, Loss on train: 0.008599596098065376\n",
            "Starting epoch 57/70, LR = [0.4]\n",
            "--- Epoch 57, Loss on train: 0.009965633973479271\n",
            "Starting epoch 58/70, LR = [0.4]\n",
            "--- Epoch 58, Loss on train: 0.010945132933557034\n",
            "Starting epoch 59/70, LR = [0.4]\n",
            "--- Epoch 59, Loss on train: 0.0070861633867025375\n",
            "Starting epoch 60/70, LR = [0.4]\n",
            "--- Epoch 60, Loss on train: 0.008271444588899612\n",
            "Starting epoch 61/70, LR = [0.4]\n",
            "--- Epoch 61, Loss on train: 0.008679076097905636\n",
            "Starting epoch 62/70, LR = [0.4]\n",
            "--- Epoch 62, Loss on train: 0.007024724502116442\n",
            "Starting epoch 63/70, LR = [0.4]\n",
            "--- Epoch 63, Loss on train: 0.008465432561933994\n",
            "Starting epoch 64/70, LR = [0.016000000000000004]\n",
            "--- Epoch 64, Loss on train: 0.004840726498514414\n",
            "Starting epoch 65/70, LR = [0.08000000000000002]\n",
            "--- Epoch 65, Loss on train: 0.006178549490869045\n",
            "Starting epoch 66/70, LR = [0.08000000000000002]\n",
            "--- Epoch 66, Loss on train: 0.005701565183699131\n",
            "Starting epoch 67/70, LR = [0.08000000000000002]\n",
            "--- Epoch 67, Loss on train: 0.006182028446346521\n",
            "Starting epoch 68/70, LR = [0.08000000000000002]\n",
            "--- Epoch 68, Loss on train: 0.005675822496414185\n",
            "Starting epoch 69/70, LR = [0.08000000000000002]\n",
            "--- Epoch 69, Loss on train: 0.0070769330486655235\n",
            "Starting epoch 70/70, LR = [0.08000000000000002]\n",
            "--- Epoch 70, Loss on train: 0.00544207775965333\n",
            "Loss on eval (group): 0.0001972736989458402\n",
            "Accuracy on eval (group): 0.741\n",
            "Accuracy on train (group): 0.9300666666666667\n",
            "******************************\n",
            "NEW GROUP OF CLASSES 7¬∞/10\n",
            "Training set length: 35000\n",
            "Test set length: 7000\n",
            "Starting epoch 1/70, LR = [2]\n",
            "--- Initial loss on train: 0.7353712320327759\n",
            "--- Epoch 1, Loss on train: 0.04461447149515152\n",
            "Starting epoch 2/70, LR = [2]\n",
            "--- Epoch 2, Loss on train: 0.041693199425935745\n",
            "Starting epoch 3/70, LR = [2]\n",
            "--- Epoch 3, Loss on train: 0.03966635465621948\n",
            "Starting epoch 4/70, LR = [2]\n",
            "--- Epoch 4, Loss on train: 0.03872961550951004\n",
            "Starting epoch 5/70, LR = [2]\n",
            "--- Epoch 5, Loss on train: 0.033809199929237366\n",
            "Starting epoch 6/70, LR = [2]\n",
            "--- Epoch 6, Loss on train: 0.03495184704661369\n",
            "Starting epoch 7/70, LR = [2]\n",
            "--- Epoch 7, Loss on train: 0.030576853081583977\n",
            "Starting epoch 8/70, LR = [2]\n",
            "--- Epoch 8, Loss on train: 0.028170185163617134\n",
            "Starting epoch 9/70, LR = [2]\n",
            "--- Epoch 9, Loss on train: 0.026120714843273163\n",
            "Starting epoch 10/70, LR = [2]\n",
            "--- Epoch 10, Loss on train: 0.027096828445792198\n",
            "Starting epoch 11/70, LR = [2]\n",
            "--- Epoch 11, Loss on train: 0.025313761085271835\n",
            "Starting epoch 12/70, LR = [2]\n",
            "--- Epoch 12, Loss on train: 0.02410149946808815\n",
            "Starting epoch 13/70, LR = [2]\n",
            "--- Epoch 13, Loss on train: 0.023482026532292366\n",
            "Starting epoch 14/70, LR = [2]\n",
            "--- Epoch 14, Loss on train: 0.02397868037223816\n",
            "Starting epoch 15/70, LR = [2]\n",
            "--- Epoch 15, Loss on train: 0.023807484656572342\n",
            "Starting epoch 16/70, LR = [2]\n",
            "--- Epoch 16, Loss on train: 0.023819293826818466\n",
            "Starting epoch 17/70, LR = [2]\n",
            "--- Epoch 17, Loss on train: 0.02008061297237873\n",
            "Starting epoch 18/70, LR = [2]\n",
            "--- Epoch 18, Loss on train: 0.023488188162446022\n",
            "Starting epoch 19/70, LR = [2]\n",
            "--- Epoch 19, Loss on train: 0.02270955964922905\n",
            "Starting epoch 20/70, LR = [2]\n",
            "--- Epoch 20, Loss on train: 0.023607904091477394\n",
            "Starting epoch 21/70, LR = [2]\n",
            "--- Epoch 21, Loss on train: 0.022746115922927856\n",
            "Starting epoch 22/70, LR = [2]\n",
            "--- Epoch 22, Loss on train: 0.02055423893034458\n",
            "Starting epoch 23/70, LR = [2]\n",
            "--- Epoch 23, Loss on train: 0.019045425578951836\n",
            "Starting epoch 24/70, LR = [2]\n",
            "--- Epoch 24, Loss on train: 0.01803833246231079\n",
            "Starting epoch 25/70, LR = [2]\n",
            "--- Epoch 25, Loss on train: 0.01659155637025833\n",
            "Starting epoch 26/70, LR = [2]\n",
            "--- Epoch 26, Loss on train: 0.01915818825364113\n",
            "Starting epoch 27/70, LR = [2]\n",
            "--- Epoch 27, Loss on train: 0.021831531077623367\n",
            "Starting epoch 28/70, LR = [2]\n",
            "--- Epoch 28, Loss on train: 0.018765009939670563\n",
            "Starting epoch 29/70, LR = [2]\n",
            "--- Epoch 29, Loss on train: 0.01899704709649086\n",
            "Starting epoch 30/70, LR = [2]\n",
            "--- Epoch 30, Loss on train: 0.01959109678864479\n",
            "Starting epoch 31/70, LR = [2]\n",
            "--- Epoch 31, Loss on train: 0.01776692271232605\n",
            "Starting epoch 32/70, LR = [2]\n",
            "--- Epoch 32, Loss on train: 0.018748300150036812\n",
            "Starting epoch 33/70, LR = [2]\n",
            "--- Epoch 33, Loss on train: 0.01693625934422016\n",
            "Starting epoch 34/70, LR = [2]\n",
            "--- Epoch 34, Loss on train: 0.016763996332883835\n",
            "Starting epoch 35/70, LR = [2]\n",
            "--- Epoch 35, Loss on train: 0.016516439616680145\n",
            "Starting epoch 36/70, LR = [2]\n",
            "--- Epoch 36, Loss on train: 0.016519011929631233\n",
            "Starting epoch 37/70, LR = [2]\n",
            "--- Epoch 37, Loss on train: 0.018358157947659492\n",
            "Starting epoch 38/70, LR = [2]\n",
            "--- Epoch 38, Loss on train: 0.016982533037662506\n",
            "Starting epoch 39/70, LR = [2]\n",
            "--- Epoch 39, Loss on train: 0.017355112358927727\n",
            "Starting epoch 40/70, LR = [2]\n",
            "--- Epoch 40, Loss on train: 0.01905854605138302\n",
            "Starting epoch 41/70, LR = [2]\n",
            "--- Epoch 41, Loss on train: 0.017009826377034187\n",
            "Starting epoch 42/70, LR = [2]\n",
            "--- Epoch 42, Loss on train: 0.017201628535985947\n",
            "Starting epoch 43/70, LR = [2]\n",
            "--- Epoch 43, Loss on train: 0.014426810666918755\n",
            "Starting epoch 44/70, LR = [2]\n",
            "--- Epoch 44, Loss on train: 0.015589194372296333\n",
            "Starting epoch 45/70, LR = [2]\n",
            "--- Epoch 45, Loss on train: 0.01758016087114811\n",
            "Starting epoch 46/70, LR = [2]\n",
            "--- Epoch 46, Loss on train: 0.01639772206544876\n",
            "Starting epoch 47/70, LR = [2]\n",
            "--- Epoch 47, Loss on train: 0.016992874443531036\n",
            "Starting epoch 48/70, LR = [2]\n",
            "--- Epoch 48, Loss on train: 0.015423178672790527\n",
            "Starting epoch 49/70, LR = [2]\n",
            "--- Epoch 49, Loss on train: 0.019932599738240242\n",
            "Starting epoch 50/70, LR = [0.08000000000000002]\n",
            "--- Epoch 50, Loss on train: 0.01504331175237894\n",
            "Starting epoch 51/70, LR = [0.4]\n",
            "--- Epoch 51, Loss on train: 0.009340687654912472\n",
            "Starting epoch 52/70, LR = [0.4]\n",
            "--- Epoch 52, Loss on train: 0.008889985270798206\n",
            "Starting epoch 53/70, LR = [0.4]\n",
            "--- Epoch 53, Loss on train: 0.009312001056969166\n",
            "Starting epoch 54/70, LR = [0.4]\n",
            "--- Epoch 54, Loss on train: 0.010343322530388832\n",
            "Starting epoch 55/70, LR = [0.4]\n",
            "--- Epoch 55, Loss on train: 0.010615027509629726\n",
            "Starting epoch 56/70, LR = [0.4]\n",
            "--- Epoch 56, Loss on train: 0.009514444507658482\n",
            "Starting epoch 57/70, LR = [0.4]\n",
            "--- Epoch 57, Loss on train: 0.010854222811758518\n",
            "Starting epoch 58/70, LR = [0.4]\n",
            "--- Epoch 58, Loss on train: 0.008340414613485336\n",
            "Starting epoch 59/70, LR = [0.4]\n",
            "--- Epoch 59, Loss on train: 0.008830239996314049\n",
            "Starting epoch 60/70, LR = [0.4]\n",
            "--- Epoch 60, Loss on train: 0.009796508587896824\n",
            "Starting epoch 61/70, LR = [0.4]\n",
            "--- Epoch 61, Loss on train: 0.010061590000987053\n",
            "Starting epoch 62/70, LR = [0.4]\n",
            "--- Epoch 62, Loss on train: 0.008790446445345879\n",
            "Starting epoch 63/70, LR = [0.4]\n",
            "--- Epoch 63, Loss on train: 0.009934419766068459\n",
            "Starting epoch 64/70, LR = [0.016000000000000004]\n",
            "--- Epoch 64, Loss on train: 0.007880670949816704\n",
            "Starting epoch 65/70, LR = [0.08000000000000002]\n",
            "--- Epoch 65, Loss on train: 0.006386389955878258\n",
            "Starting epoch 66/70, LR = [0.08000000000000002]\n",
            "--- Epoch 66, Loss on train: 0.0055231754668056965\n",
            "Starting epoch 67/70, LR = [0.08000000000000002]\n",
            "--- Epoch 67, Loss on train: 0.0073827942833304405\n",
            "Starting epoch 68/70, LR = [0.08000000000000002]\n",
            "--- Epoch 68, Loss on train: 0.0065706572495400906\n",
            "Starting epoch 69/70, LR = [0.08000000000000002]\n",
            "--- Epoch 69, Loss on train: 0.008198539726436138\n",
            "Starting epoch 70/70, LR = [0.08000000000000002]\n",
            "--- Epoch 70, Loss on train: 0.0071079847402870655\n",
            "Loss on eval (group): 0.00017356890679470131\n",
            "Accuracy on eval (group): 0.7231428571428572\n",
            "Accuracy on train (group): 0.9004857142857143\n",
            "******************************\n",
            "NEW GROUP OF CLASSES 8¬∞/10\n",
            "Training set length: 40000\n",
            "Test set length: 8000\n",
            "Starting epoch 1/70, LR = [2]\n",
            "--- Initial loss on train: 0.7220431566238403\n",
            "--- Epoch 1, Loss on train: 0.04708205163478851\n",
            "Starting epoch 2/70, LR = [2]\n",
            "--- Epoch 2, Loss on train: 0.0456279031932354\n",
            "Starting epoch 3/70, LR = [2]\n",
            "--- Epoch 3, Loss on train: 0.04169033095240593\n",
            "Starting epoch 4/70, LR = [2]\n",
            "--- Epoch 4, Loss on train: 0.03909386694431305\n",
            "Starting epoch 5/70, LR = [2]\n",
            "--- Epoch 5, Loss on train: 0.03752453252673149\n",
            "Starting epoch 6/70, LR = [2]\n",
            "--- Epoch 6, Loss on train: 0.03170476481318474\n",
            "Starting epoch 7/70, LR = [2]\n",
            "--- Epoch 7, Loss on train: 0.033010222017765045\n",
            "Starting epoch 8/70, LR = [2]\n",
            "--- Epoch 8, Loss on train: 0.032098300755023956\n",
            "Starting epoch 9/70, LR = [2]\n",
            "--- Epoch 9, Loss on train: 0.030718207359313965\n",
            "Starting epoch 10/70, LR = [2]\n",
            "--- Epoch 10, Loss on train: 0.028839511796832085\n",
            "Starting epoch 11/70, LR = [2]\n",
            "--- Epoch 11, Loss on train: 0.026257378980517387\n",
            "Starting epoch 12/70, LR = [2]\n",
            "--- Epoch 12, Loss on train: 0.025465015321969986\n",
            "Starting epoch 13/70, LR = [2]\n",
            "--- Epoch 13, Loss on train: 0.022127505391836166\n",
            "Starting epoch 14/70, LR = [2]\n",
            "--- Epoch 14, Loss on train: 0.022364726290106773\n",
            "Starting epoch 15/70, LR = [2]\n",
            "--- Epoch 15, Loss on train: 0.02621447481215\n",
            "Starting epoch 16/70, LR = [2]\n",
            "--- Epoch 16, Loss on train: 0.02481725439429283\n",
            "Starting epoch 17/70, LR = [2]\n",
            "--- Epoch 17, Loss on train: 0.024036025628447533\n",
            "Starting epoch 18/70, LR = [2]\n",
            "--- Epoch 18, Loss on train: 0.022200211882591248\n",
            "Starting epoch 19/70, LR = [2]\n",
            "--- Epoch 19, Loss on train: 0.01967427507042885\n",
            "Starting epoch 20/70, LR = [2]\n",
            "--- Epoch 20, Loss on train: 0.02185172773897648\n",
            "Starting epoch 21/70, LR = [2]\n",
            "--- Epoch 21, Loss on train: 0.021055664867162704\n",
            "Starting epoch 22/70, LR = [2]\n",
            "--- Epoch 22, Loss on train: 0.022255372256040573\n",
            "Starting epoch 23/70, LR = [2]\n",
            "--- Epoch 23, Loss on train: 0.020937122404575348\n",
            "Starting epoch 24/70, LR = [2]\n",
            "--- Epoch 24, Loss on train: 0.020775241777300835\n",
            "Starting epoch 25/70, LR = [2]\n",
            "--- Epoch 25, Loss on train: 0.021492023020982742\n",
            "Starting epoch 26/70, LR = [2]\n",
            "--- Epoch 26, Loss on train: 0.022403087466955185\n",
            "Starting epoch 27/70, LR = [2]\n",
            "--- Epoch 27, Loss on train: 0.02084183320403099\n",
            "Starting epoch 28/70, LR = [2]\n",
            "--- Epoch 28, Loss on train: 0.01986645720899105\n",
            "Starting epoch 29/70, LR = [2]\n",
            "--- Epoch 29, Loss on train: 0.019661208614706993\n",
            "Starting epoch 30/70, LR = [2]\n",
            "--- Epoch 30, Loss on train: 0.018388913944363594\n",
            "Starting epoch 31/70, LR = [2]\n",
            "--- Epoch 31, Loss on train: 0.020374998450279236\n",
            "Starting epoch 32/70, LR = [2]\n",
            "--- Epoch 32, Loss on train: 0.019064050167798996\n",
            "Starting epoch 33/70, LR = [2]\n",
            "--- Epoch 33, Loss on train: 0.01972331665456295\n",
            "Starting epoch 34/70, LR = [2]\n",
            "--- Epoch 34, Loss on train: 0.018452951684594154\n",
            "Starting epoch 35/70, LR = [2]\n",
            "--- Epoch 35, Loss on train: 0.0187908373773098\n",
            "Starting epoch 36/70, LR = [2]\n",
            "--- Epoch 36, Loss on train: 0.0192918349057436\n",
            "Starting epoch 37/70, LR = [2]\n",
            "--- Epoch 37, Loss on train: 0.01846025511622429\n",
            "Starting epoch 38/70, LR = [2]\n",
            "--- Epoch 38, Loss on train: 0.019901234656572342\n",
            "Starting epoch 39/70, LR = [2]\n",
            "--- Epoch 39, Loss on train: 0.01822919026017189\n",
            "Starting epoch 40/70, LR = [2]\n",
            "--- Epoch 40, Loss on train: 0.019495327025651932\n",
            "Starting epoch 41/70, LR = [2]\n",
            "--- Epoch 41, Loss on train: 0.01680840365588665\n",
            "Starting epoch 42/70, LR = [2]\n",
            "--- Epoch 42, Loss on train: 0.019281312823295593\n",
            "Starting epoch 43/70, LR = [2]\n",
            "--- Epoch 43, Loss on train: 0.019471609964966774\n",
            "Starting epoch 44/70, LR = [2]\n",
            "--- Epoch 44, Loss on train: 0.017956716939806938\n",
            "Starting epoch 45/70, LR = [2]\n",
            "--- Epoch 45, Loss on train: 0.020733952522277832\n",
            "Starting epoch 46/70, LR = [2]\n",
            "--- Epoch 46, Loss on train: 0.01813567243516445\n",
            "Starting epoch 47/70, LR = [2]\n",
            "--- Epoch 47, Loss on train: 0.014672788791358471\n",
            "Starting epoch 48/70, LR = [2]\n",
            "--- Epoch 48, Loss on train: 0.013488087803125381\n",
            "Starting epoch 49/70, LR = [2]\n",
            "--- Epoch 49, Loss on train: 0.02033454366028309\n",
            "Starting epoch 50/70, LR = [0.08000000000000002]\n",
            "--- Epoch 50, Loss on train: 0.013908352702856064\n",
            "Starting epoch 51/70, LR = [0.4]\n",
            "--- Epoch 51, Loss on train: 0.012348444201052189\n",
            "Starting epoch 52/70, LR = [0.4]\n",
            "--- Epoch 52, Loss on train: 0.011501103639602661\n",
            "Starting epoch 53/70, LR = [0.4]\n",
            "--- Epoch 53, Loss on train: 0.011492304503917694\n",
            "Starting epoch 54/70, LR = [0.4]\n",
            "--- Epoch 54, Loss on train: 0.010693779215216637\n",
            "Starting epoch 55/70, LR = [0.4]\n",
            "--- Epoch 55, Loss on train: 0.010978379286825657\n",
            "Starting epoch 56/70, LR = [0.4]\n",
            "--- Epoch 56, Loss on train: 0.011389984749257565\n",
            "Starting epoch 57/70, LR = [0.4]\n",
            "--- Epoch 57, Loss on train: 0.011264625936746597\n",
            "Starting epoch 58/70, LR = [0.4]\n",
            "--- Epoch 58, Loss on train: 0.012361998669803143\n",
            "Starting epoch 59/70, LR = [0.4]\n",
            "--- Epoch 59, Loss on train: 0.012379438616335392\n",
            "Starting epoch 60/70, LR = [0.4]\n",
            "--- Epoch 60, Loss on train: 0.00992126576602459\n",
            "Starting epoch 61/70, LR = [0.4]\n",
            "--- Epoch 61, Loss on train: 0.011222300119698048\n",
            "Starting epoch 62/70, LR = [0.4]\n",
            "--- Epoch 62, Loss on train: 0.009434829466044903\n",
            "Starting epoch 63/70, LR = [0.4]\n",
            "--- Epoch 63, Loss on train: 0.00974192563444376\n",
            "Starting epoch 64/70, LR = [0.016000000000000004]\n",
            "--- Epoch 64, Loss on train: 0.007583560887724161\n",
            "Starting epoch 65/70, LR = [0.08000000000000002]\n",
            "--- Epoch 65, Loss on train: 0.008403602056205273\n",
            "Starting epoch 66/70, LR = [0.08000000000000002]\n",
            "--- Epoch 66, Loss on train: 0.00797245278954506\n",
            "Starting epoch 67/70, LR = [0.08000000000000002]\n",
            "--- Epoch 67, Loss on train: 0.007117982488125563\n",
            "Starting epoch 68/70, LR = [0.08000000000000002]\n",
            "--- Epoch 68, Loss on train: 0.008869330398738384\n",
            "Starting epoch 69/70, LR = [0.08000000000000002]\n",
            "--- Epoch 69, Loss on train: 0.006415080279111862\n",
            "Starting epoch 70/70, LR = [0.08000000000000002]\n",
            "--- Epoch 70, Loss on train: 0.007128571160137653\n",
            "Loss on eval (group): 0.00016008116875309498\n",
            "Accuracy on eval (group): 0.7055\n",
            "Accuracy on train (group): 0.882925\n",
            "******************************\n",
            "NEW GROUP OF CLASSES 9¬∞/10\n",
            "Training set length: 45000\n",
            "Test set length: 9000\n",
            "Starting epoch 1/70, LR = [2]\n",
            "--- Initial loss on train: 0.7375350594520569\n",
            "--- Epoch 1, Loss on train: 0.04737074673175812\n",
            "Starting epoch 2/70, LR = [2]\n",
            "--- Epoch 2, Loss on train: 0.045064300298690796\n",
            "Starting epoch 3/70, LR = [2]\n",
            "--- Epoch 3, Loss on train: 0.043329209089279175\n",
            "Starting epoch 4/70, LR = [2]\n",
            "--- Epoch 4, Loss on train: 0.03794392943382263\n",
            "Starting epoch 5/70, LR = [2]\n",
            "--- Epoch 5, Loss on train: 0.03691522032022476\n",
            "Starting epoch 6/70, LR = [2]\n",
            "--- Epoch 6, Loss on train: 0.03572447597980499\n",
            "Starting epoch 7/70, LR = [2]\n",
            "--- Epoch 7, Loss on train: 0.03470870852470398\n",
            "Starting epoch 8/70, LR = [2]\n",
            "--- Epoch 8, Loss on train: 0.03147580847144127\n",
            "Starting epoch 9/70, LR = [2]\n",
            "--- Epoch 9, Loss on train: 0.03290082886815071\n",
            "Starting epoch 10/70, LR = [2]\n",
            "--- Epoch 10, Loss on train: 0.029697580263018608\n",
            "Starting epoch 11/70, LR = [2]\n",
            "--- Epoch 11, Loss on train: 0.029766391962766647\n",
            "Starting epoch 12/70, LR = [2]\n",
            "--- Epoch 12, Loss on train: 0.02755802869796753\n",
            "Starting epoch 13/70, LR = [2]\n",
            "--- Epoch 13, Loss on train: 0.025305554270744324\n",
            "Starting epoch 14/70, LR = [2]\n",
            "--- Epoch 14, Loss on train: 0.025635480880737305\n",
            "Starting epoch 15/70, LR = [2]\n",
            "--- Epoch 15, Loss on train: 0.02364283986389637\n",
            "Starting epoch 16/70, LR = [2]\n",
            "--- Epoch 16, Loss on train: 0.023934559896588326\n",
            "Starting epoch 17/70, LR = [2]\n",
            "--- Epoch 17, Loss on train: 0.024124622344970703\n",
            "Starting epoch 18/70, LR = [2]\n",
            "--- Epoch 18, Loss on train: 0.02310321480035782\n",
            "Starting epoch 19/70, LR = [2]\n",
            "--- Epoch 19, Loss on train: 0.023728741332888603\n",
            "Starting epoch 20/70, LR = [2]\n",
            "--- Epoch 20, Loss on train: 0.022245697677135468\n",
            "Starting epoch 21/70, LR = [2]\n",
            "--- Epoch 21, Loss on train: 0.02413650043308735\n",
            "Starting epoch 22/70, LR = [2]\n",
            "--- Epoch 22, Loss on train: 0.023841017857193947\n",
            "Starting epoch 23/70, LR = [2]\n",
            "--- Epoch 23, Loss on train: 0.022453593090176582\n",
            "Starting epoch 24/70, LR = [2]\n",
            "--- Epoch 24, Loss on train: 0.02208486944437027\n",
            "Starting epoch 25/70, LR = [2]\n",
            "--- Epoch 25, Loss on train: 0.02017407864332199\n",
            "Starting epoch 26/70, LR = [2]\n",
            "--- Epoch 26, Loss on train: 0.0219877902418375\n",
            "Starting epoch 27/70, LR = [2]\n",
            "--- Epoch 27, Loss on train: 0.021601993590593338\n",
            "Starting epoch 28/70, LR = [2]\n",
            "--- Epoch 28, Loss on train: 0.02108713984489441\n",
            "Starting epoch 29/70, LR = [2]\n",
            "--- Epoch 29, Loss on train: 0.02150529995560646\n",
            "Starting epoch 30/70, LR = [2]\n",
            "--- Epoch 30, Loss on train: 0.020790763199329376\n",
            "Starting epoch 31/70, LR = [2]\n",
            "--- Epoch 31, Loss on train: 0.020503396168351173\n",
            "Starting epoch 32/70, LR = [2]\n",
            "--- Epoch 32, Loss on train: 0.01990886963903904\n",
            "Starting epoch 33/70, LR = [2]\n",
            "--- Epoch 33, Loss on train: 0.018680572509765625\n",
            "Starting epoch 34/70, LR = [2]\n",
            "--- Epoch 34, Loss on train: 0.018886689096689224\n",
            "Starting epoch 35/70, LR = [2]\n",
            "--- Epoch 35, Loss on train: 0.020371705293655396\n",
            "Starting epoch 36/70, LR = [2]\n",
            "--- Epoch 36, Loss on train: 0.025280723348259926\n",
            "Starting epoch 37/70, LR = [2]\n",
            "--- Epoch 37, Loss on train: 0.02285073697566986\n",
            "Starting epoch 38/70, LR = [2]\n",
            "--- Epoch 38, Loss on train: 0.019945096224546432\n",
            "Starting epoch 39/70, LR = [2]\n",
            "--- Epoch 39, Loss on train: 0.0188114196062088\n",
            "Starting epoch 40/70, LR = [2]\n",
            "--- Epoch 40, Loss on train: 0.0186134222894907\n",
            "Starting epoch 41/70, LR = [2]\n",
            "--- Epoch 41, Loss on train: 0.02093883976340294\n",
            "Starting epoch 42/70, LR = [2]\n",
            "--- Epoch 42, Loss on train: 0.018765244632959366\n",
            "Starting epoch 43/70, LR = [2]\n",
            "--- Epoch 43, Loss on train: 0.019633648917078972\n",
            "Starting epoch 44/70, LR = [2]\n",
            "--- Epoch 44, Loss on train: 0.01977185718715191\n",
            "Starting epoch 45/70, LR = [2]\n",
            "--- Epoch 45, Loss on train: 0.019463317468762398\n",
            "Starting epoch 46/70, LR = [2]\n",
            "--- Epoch 46, Loss on train: 0.016967177391052246\n",
            "Starting epoch 47/70, LR = [2]\n",
            "--- Epoch 47, Loss on train: 0.01737927831709385\n",
            "Starting epoch 48/70, LR = [2]\n",
            "--- Epoch 48, Loss on train: 0.02118094451725483\n",
            "Starting epoch 49/70, LR = [2]\n",
            "--- Epoch 49, Loss on train: 0.02109343558549881\n",
            "Starting epoch 50/70, LR = [0.08000000000000002]\n",
            "--- Epoch 50, Loss on train: 0.013502700254321098\n",
            "Starting epoch 51/70, LR = [0.4]\n",
            "--- Epoch 51, Loss on train: 0.015086131170392036\n",
            "Starting epoch 52/70, LR = [0.4]\n",
            "--- Epoch 52, Loss on train: 0.01308386493474245\n",
            "Starting epoch 53/70, LR = [0.4]\n",
            "--- Epoch 53, Loss on train: 0.011934630572795868\n",
            "Starting epoch 54/70, LR = [0.4]\n",
            "--- Epoch 54, Loss on train: 0.013712665997445583\n",
            "Starting epoch 55/70, LR = [0.4]\n",
            "--- Epoch 55, Loss on train: 0.011828835122287273\n",
            "Starting epoch 56/70, LR = [0.4]\n",
            "--- Epoch 56, Loss on train: 0.013462974689900875\n",
            "Starting epoch 57/70, LR = [0.4]\n",
            "--- Epoch 57, Loss on train: 0.012953363358974457\n",
            "Starting epoch 58/70, LR = [0.4]\n",
            "--- Epoch 58, Loss on train: 0.012332547456026077\n",
            "Starting epoch 59/70, LR = [0.4]\n",
            "--- Epoch 59, Loss on train: 0.011609707027673721\n",
            "Starting epoch 60/70, LR = [0.4]\n",
            "--- Epoch 60, Loss on train: 0.011475393548607826\n",
            "Starting epoch 61/70, LR = [0.4]\n",
            "--- Epoch 61, Loss on train: 0.01276906207203865\n",
            "Starting epoch 62/70, LR = [0.4]\n",
            "--- Epoch 62, Loss on train: 0.013130883686244488\n",
            "Starting epoch 63/70, LR = [0.4]\n",
            "--- Epoch 63, Loss on train: 0.012623965740203857\n",
            "Starting epoch 64/70, LR = [0.016000000000000004]\n",
            "--- Epoch 64, Loss on train: 0.008703668601810932\n",
            "Starting epoch 65/70, LR = [0.08000000000000002]\n",
            "--- Epoch 65, Loss on train: 0.01182390097528696\n",
            "Starting epoch 66/70, LR = [0.08000000000000002]\n",
            "--- Epoch 66, Loss on train: 0.008488579653203487\n",
            "Starting epoch 67/70, LR = [0.08000000000000002]\n",
            "--- Epoch 67, Loss on train: 0.011374971829354763\n",
            "Starting epoch 68/70, LR = [0.08000000000000002]\n",
            "--- Epoch 68, Loss on train: 0.009731926955282688\n",
            "Starting epoch 69/70, LR = [0.08000000000000002]\n",
            "--- Epoch 69, Loss on train: 0.010802525095641613\n",
            "Starting epoch 70/70, LR = [0.08000000000000002]\n",
            "--- Epoch 70, Loss on train: 0.008795751258730888\n",
            "Loss on eval (group): 0.00014836510539882714\n",
            "Accuracy on eval (group): 0.6962222222222222\n",
            "Accuracy on train (group): 0.8652\n",
            "******************************\n",
            "NEW GROUP OF CLASSES 10¬∞/10\n",
            "Training set length: 50000\n",
            "Test set length: 10000\n",
            "Starting epoch 1/70, LR = [2]\n",
            "--- Initial loss on train: 0.8390365242958069\n",
            "--- Epoch 1, Loss on train: 0.05046536773443222\n",
            "Starting epoch 2/70, LR = [2]\n",
            "--- Epoch 2, Loss on train: 0.04729530215263367\n",
            "Starting epoch 3/70, LR = [2]\n",
            "--- Epoch 3, Loss on train: 0.04450620710849762\n",
            "Starting epoch 4/70, LR = [2]\n",
            "--- Epoch 4, Loss on train: 0.03997700661420822\n",
            "Starting epoch 5/70, LR = [2]\n",
            "--- Epoch 5, Loss on train: 0.03753051534295082\n",
            "Starting epoch 6/70, LR = [2]\n",
            "--- Epoch 6, Loss on train: 0.03698932006955147\n",
            "Starting epoch 7/70, LR = [2]\n",
            "--- Epoch 7, Loss on train: 0.03775864467024803\n",
            "Starting epoch 8/70, LR = [2]\n",
            "--- Epoch 8, Loss on train: 0.03188949078321457\n",
            "Starting epoch 9/70, LR = [2]\n",
            "--- Epoch 9, Loss on train: 0.03190774843096733\n",
            "Starting epoch 10/70, LR = [2]\n",
            "--- Epoch 10, Loss on train: 0.03007473237812519\n",
            "Starting epoch 11/70, LR = [2]\n",
            "--- Epoch 11, Loss on train: 0.03066824935376644\n",
            "Starting epoch 12/70, LR = [2]\n",
            "--- Epoch 12, Loss on train: 0.02577250264585018\n",
            "Starting epoch 13/70, LR = [2]\n",
            "--- Epoch 13, Loss on train: 0.030368663370609283\n",
            "Starting epoch 14/70, LR = [2]\n",
            "--- Epoch 14, Loss on train: 0.02696102112531662\n",
            "Starting epoch 15/70, LR = [2]\n",
            "--- Epoch 15, Loss on train: 0.025160811841487885\n",
            "Starting epoch 16/70, LR = [2]\n",
            "--- Epoch 16, Loss on train: 0.026210125535726547\n",
            "Starting epoch 17/70, LR = [2]\n",
            "--- Epoch 17, Loss on train: 0.024401487782597542\n",
            "Starting epoch 18/70, LR = [2]\n",
            "--- Epoch 18, Loss on train: 0.023828838020563126\n",
            "Starting epoch 19/70, LR = [2]\n",
            "--- Epoch 19, Loss on train: 0.025636034086346626\n",
            "Starting epoch 20/70, LR = [2]\n",
            "--- Epoch 20, Loss on train: 0.02261154167354107\n",
            "Starting epoch 21/70, LR = [2]\n",
            "--- Epoch 21, Loss on train: 0.024368686601519585\n",
            "Starting epoch 22/70, LR = [2]\n",
            "--- Epoch 22, Loss on train: 0.024611523374915123\n",
            "Starting epoch 23/70, LR = [2]\n",
            "--- Epoch 23, Loss on train: 0.023178299888968468\n",
            "Starting epoch 24/70, LR = [2]\n",
            "--- Epoch 24, Loss on train: 0.025268159806728363\n",
            "Starting epoch 25/70, LR = [2]\n",
            "--- Epoch 25, Loss on train: 0.022677510976791382\n",
            "Starting epoch 26/70, LR = [2]\n",
            "--- Epoch 26, Loss on train: 0.022995268926024437\n",
            "Starting epoch 27/70, LR = [2]\n",
            "--- Epoch 27, Loss on train: 0.022484660148620605\n",
            "Starting epoch 28/70, LR = [2]\n",
            "--- Epoch 28, Loss on train: 0.019523438066244125\n",
            "Starting epoch 29/70, LR = [2]\n",
            "--- Epoch 29, Loss on train: 0.022267760708928108\n",
            "Starting epoch 30/70, LR = [2]\n",
            "--- Epoch 30, Loss on train: 0.022033123299479485\n",
            "Starting epoch 31/70, LR = [2]\n",
            "--- Epoch 31, Loss on train: 0.018730154260993004\n",
            "Starting epoch 32/70, LR = [2]\n",
            "--- Epoch 32, Loss on train: 0.018963459879159927\n",
            "Starting epoch 33/70, LR = [2]\n",
            "--- Epoch 33, Loss on train: 0.02152988873422146\n",
            "Starting epoch 34/70, LR = [2]\n",
            "--- Epoch 34, Loss on train: 0.023097967728972435\n",
            "Starting epoch 35/70, LR = [2]\n",
            "--- Epoch 35, Loss on train: 0.019908253103494644\n",
            "Starting epoch 36/70, LR = [2]\n",
            "--- Epoch 36, Loss on train: 0.02339593507349491\n",
            "Starting epoch 37/70, LR = [2]\n",
            "--- Epoch 37, Loss on train: 0.017793674021959305\n",
            "Starting epoch 38/70, LR = [2]\n",
            "--- Epoch 38, Loss on train: 0.020491568371653557\n",
            "Starting epoch 39/70, LR = [2]\n",
            "--- Epoch 39, Loss on train: 0.02078152634203434\n",
            "Starting epoch 40/70, LR = [2]\n",
            "--- Epoch 40, Loss on train: 0.020076904445886612\n",
            "Starting epoch 41/70, LR = [2]\n",
            "--- Epoch 41, Loss on train: 0.01788984425365925\n",
            "Starting epoch 42/70, LR = [2]\n",
            "--- Epoch 42, Loss on train: 0.019197111949324608\n",
            "Starting epoch 43/70, LR = [2]\n",
            "--- Epoch 43, Loss on train: 0.02117188461124897\n",
            "Starting epoch 44/70, LR = [2]\n",
            "--- Epoch 44, Loss on train: 0.02259035035967827\n",
            "Starting epoch 45/70, LR = [2]\n",
            "--- Epoch 45, Loss on train: 0.0207472313195467\n",
            "Starting epoch 46/70, LR = [2]\n",
            "--- Epoch 46, Loss on train: 0.021306395530700684\n",
            "Starting epoch 47/70, LR = [2]\n",
            "--- Epoch 47, Loss on train: 0.020729245617985725\n",
            "Starting epoch 48/70, LR = [2]\n",
            "--- Epoch 48, Loss on train: 0.022961337119340897\n",
            "Starting epoch 49/70, LR = [2]\n",
            "--- Epoch 49, Loss on train: 0.02121264860033989\n",
            "Starting epoch 50/70, LR = [0.08000000000000002]\n",
            "--- Epoch 50, Loss on train: 0.015123879536986351\n",
            "Starting epoch 51/70, LR = [0.4]\n",
            "--- Epoch 51, Loss on train: 0.014448079280555248\n",
            "Starting epoch 52/70, LR = [0.4]\n",
            "--- Epoch 52, Loss on train: 0.013635687530040741\n",
            "Starting epoch 53/70, LR = [0.4]\n",
            "--- Epoch 53, Loss on train: 0.01548423059284687\n",
            "Starting epoch 54/70, LR = [0.4]\n",
            "--- Epoch 54, Loss on train: 0.013082224875688553\n",
            "Starting epoch 55/70, LR = [0.4]\n",
            "--- Epoch 55, Loss on train: 0.015314768068492413\n",
            "Starting epoch 56/70, LR = [0.4]\n",
            "--- Epoch 56, Loss on train: 0.013632345013320446\n",
            "Starting epoch 57/70, LR = [0.4]\n",
            "--- Epoch 57, Loss on train: 0.012602385133504868\n",
            "Starting epoch 58/70, LR = [0.4]\n",
            "--- Epoch 58, Loss on train: 0.01398744061589241\n",
            "Starting epoch 59/70, LR = [0.4]\n",
            "--- Epoch 59, Loss on train: 0.01572990231215954\n",
            "Starting epoch 60/70, LR = [0.4]\n",
            "--- Epoch 60, Loss on train: 0.014240494929254055\n",
            "Starting epoch 61/70, LR = [0.4]\n",
            "--- Epoch 61, Loss on train: 0.01383278425782919\n",
            "Starting epoch 62/70, LR = [0.4]\n",
            "--- Epoch 62, Loss on train: 0.013481316156685352\n",
            "Starting epoch 63/70, LR = [0.4]\n",
            "--- Epoch 63, Loss on train: 0.012986933812499046\n",
            "Starting epoch 64/70, LR = [0.016000000000000004]\n",
            "--- Epoch 64, Loss on train: 0.011797461658716202\n",
            "Starting epoch 65/70, LR = [0.08000000000000002]\n",
            "--- Epoch 65, Loss on train: 0.010576760396361351\n",
            "Starting epoch 66/70, LR = [0.08000000000000002]\n",
            "--- Epoch 66, Loss on train: 0.009793645702302456\n",
            "Starting epoch 67/70, LR = [0.08000000000000002]\n",
            "--- Epoch 67, Loss on train: 0.010053018108010292\n",
            "Starting epoch 68/70, LR = [0.08000000000000002]\n",
            "--- Epoch 68, Loss on train: 0.012123013846576214\n",
            "Starting epoch 69/70, LR = [0.08000000000000002]\n",
            "--- Epoch 69, Loss on train: 0.009280689060688019\n",
            "Starting epoch 70/70, LR = [0.08000000000000002]\n",
            "--- Epoch 70, Loss on train: 0.009491175413131714\n",
            "Loss on eval (group): 0.00013639121307060123\n",
            "Accuracy on eval (group): 0.6878\n",
            "Accuracy on train (group): 0.84026\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "> In 308.2 minutes\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}