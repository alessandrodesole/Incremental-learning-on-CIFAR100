{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ablation Study_Losses.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9cd0a2aeaa394804914c4ec550fd7ae1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_76b76c44013c4858b5825f046dc86b58",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4d611219a5764f45ac99db6a5c11b810",
              "IPY_MODEL_123156ec1cf14659a5de3e78a55987a5"
            ]
          }
        },
        "76b76c44013c4858b5825f046dc86b58": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4d611219a5764f45ac99db6a5c11b810": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_aa2ef208a83743c49a5ce0a628909248",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f83a5c3a30f54e75b9e765c0c19784f5"
          }
        },
        "123156ec1cf14659a5de3e78a55987a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_33a2fd94a7694881917f98d01cf392ea",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 169009152/? [00:30&lt;00:00, 12891548.69it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e71eef3d073a40608a4684b340ce59ef"
          }
        },
        "aa2ef208a83743c49a5ce0a628909248": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f83a5c3a30f54e75b9e765c0c19784f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "33a2fd94a7694881917f98d01cf392ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e71eef3d073a40608a4684b340ce59ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGeu47FfPs5I",
        "colab_type": "text"
      },
      "source": [
        "### **Ablation Study**\n",
        "Classification and distillation losses:\n",
        "\n",
        "\n",
        "*   CE + MSE\n",
        "*   BCE + MSE\n",
        "*   MSE + MSE\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CwlqKl4RzLi",
        "colab_type": "text"
      },
      "source": [
        "**Install requirements**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9O3aM3Tb28q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip3 install 'torch==1.3.1'\n",
        "#!pip3 install 'torchvision==0.5.0'\n",
        "#!pip3 install 'Pillow-SIMD'\n",
        "#!pip3 install 'tqdm'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrkE-2MVbZjU",
        "colab_type": "text"
      },
      "source": [
        "**Import models and functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOGd-aASa40O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "9d860568-4262-4992-fded-09633360925c"
      },
      "source": [
        "import os\n",
        "\n",
        "if not os.path.isdir('./models'):\n",
        "  !git clone https://github.com/gabrieletiboni/Incremental-learning-on-image-recognition.git\n",
        "  !cp -r \"/content/Incremental-learning-on-image-recognition/CODE/utils\" \"/content/\"\n",
        "  !cp -r \"/content/Incremental-learning-on-image-recognition/CODE/models\" \"/content/\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Incremental-learning-on-image-recognition'...\n",
            "remote: Enumerating objects: 33, done.\u001b[K\n",
            "remote: Counting objects: 100% (33/33), done.\u001b[K\n",
            "remote: Compressing objects: 100% (33/33), done.\u001b[K\n",
            "remote: Total 2479 (delta 15), reused 0 (delta 0), pack-reused 2446\u001b[K\n",
            "Receiving objects: 100% (2479/2479), 9.73 MiB | 2.25 MiB/s, done.\n",
            "Resolving deltas: 100% (792/792), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esZgBn2rBV9L",
        "colab_type": "text"
      },
      "source": [
        "**Import libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyDs3TD9Bda1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.backends import cudnn\n",
        "\n",
        "import datetime\n",
        "from pytz import timezone\n",
        "import os\n",
        "import math\n",
        "import time\n",
        "import random\n",
        "\n",
        "\n",
        "from torchvision.datasets import CIFAR100\n",
        "from torchvision.transforms import transforms\n",
        "import torchvision.utils\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "import torch.optim as optim\n",
        "\n",
        "from models.ResNet import resnet32\n",
        "# Below a modified version that best represents the same ResNet32 used by iCaRL\n",
        "# from models.ResNet_iCaRLVersion import resnet32\n",
        "\n",
        "# from models.iCaRL import *\n",
        "from models.iCaRL import *\n",
        "from utils.utils import *\n",
        "from copy import deepcopy\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NT0Un92iJHFo",
        "colab_type": "text"
      },
      "source": [
        "**GLOBAL PARAMETERS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSHzmwaxJGEG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DEVICE = 'cuda'\n",
        "NUM_CLASSES = 100\n",
        "DATA_DIR = './CIFAR_100'\n",
        "RUNS_DIR = '/content/Incremental-learning-on-image-recognition/RUNS'\n",
        "\n",
        "# --- CUSTOM PARAMETERS\n",
        "RANDOM_STATE = 2000          # int or None (Tarantino: 'tarantino', iCaRL: 1993, Telegram: 'telegram')\n",
        "\n",
        "N_GROUPS_FOR_TRAINING = 10   # Numero di gruppi di classi da usare in fase di training (1: usa solo il primo gruppo, 10: usa tutti i gruppi di classi)\n",
        "\n",
        "USE_HERDING = False\n",
        "\n",
        "GITHUB_USER = 0             # 0: Roberto, 1: Alessandro, 2: Gabriele\n",
        "\n",
        "CIFAR_NORMALIZE = False     # If True normalizes tensor with mean and standard deviation of CIFAR 100\n",
        "\n",
        "LOSS_TYPE = 'bce_l2'        # 'bce_l2' -> tuned alpha = \n",
        "ALPHA = 100                 # 'ce_l2' -> outputs_normalization, alpha\n",
        "                            # 'l2_l2' -> keep alpha=1, change LR=[1.0, 2.0 ,10.0]\n",
        "\n",
        "METHOD = 'iCARL_bce_l2'\n",
        "# ---------------------\n",
        "\n",
        "DATA_AUGMENTATION = True\n",
        "USE_VALIDATION_SET = False\n",
        "SHUFFLE_CLASSES = True\n",
        "DUMP_FINAL_RESULTS_ON_GSPREADSHEET = True\n",
        "COMMIT_ON_GITHUB = True\n",
        "EVAL_AFTER_EACH_EPOCH = False\n",
        "BCE_VAR = 2          # 1: solo le classi attuali per il one-hot (loss divisa per 128x10, poi 128x20, etc.)\n",
        "                     # 2: usa 100 classi fin da subito nel calcolo della loss (loss divisa sempre per 128x100)\n",
        "                     # 3: usa le classi attuali per il one-hot ma dividi per 128x100 la loss\n",
        "# ----------------------------------\n",
        "\n",
        "# --- HYPERPARAMETERS\n",
        "K = 2000\n",
        "BATCH_SIZE = 128\n",
        "LR = 0.01                   # iCaRL uses LR=2 solo perchÃ¨ usa la BCE, in generale usare 0.2\n",
        "MOMENTUM = 0.9              # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
        "WEIGHT_DECAY = 1e-5         # Regularization\n",
        "\n",
        "NUM_EPOCHS = 70             # Total number of training epochs (iterations over dataset)\n",
        "DO_MULTILR_STEP_DOWN = True # step down at 7/10 and 9/10\n",
        "STEP_SIZE = 10\n",
        "GAMMA = 0.2                 # Multiplicative factor for learning rate step-down\n",
        "# ---------------------"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "As6ukzaJE0kN",
        "colab_type": "text"
      },
      "source": [
        "**Define Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e19l7N4HE6EB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if CIFAR_NORMALIZE: \n",
        "  MEANS, STDS = (0.5071, 0.4865, 0.4409), (0.2673, 0.2564, 0.2762) # Normalizes tensor with mean and standard deviation of CIFAR 100\n",
        "else: \n",
        "  MEANS, STDS = (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)\n",
        "\n",
        "# Define transforms for training phase\n",
        "if DATA_AUGMENTATION:\n",
        "\ttrain_transform = transforms.Compose([\n",
        "\t\t\t\t\t\t\t\t\t\t\ttransforms.RandomCrop(32, padding=4),\n",
        "\t\t\t\t\t\t\t\t\t\t\ttransforms.RandomHorizontalFlip(p=0.5),\n",
        "\t\t\t\t\t\t\t\t\t\t\ttransforms.ToTensor(), # Turn PIL Image to torch.Tensor\n",
        "\t\t\t\t\t\t\t\t\t\t\ttransforms.Normalize(mean=MEANS, std=STDS) # Normalizes tensor with mean and standard deviation of CIFAR 100\n",
        "\t\t\t\t\t\t\t\t\t])\n",
        "else:\n",
        "\ttrain_transform = transforms.Compose([\n",
        "\t\t\t\t\t\t\t\t\t\t\ttransforms.ToTensor(), # Turn PIL Image to torch.Tensor\n",
        "\t\t\t\t\t\t\t\t\t\t\ttransforms.Normalize(mean=MEANS, std=STDS) # Normalizes tensor with mean and standard deviation of CIFAR 100\n",
        "\t\t\t\t\t\t\t\t\t])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "\t\t\t\t\t\t\t\t\t\ttransforms.ToTensor(),\n",
        "\t\t\t\t\t\t\t\t\t\ttransforms.Normalize(mean=MEANS, std=STDS) # Normalizes tensor with mean and standard deviation of CIFAR 100                                                                                                \n",
        "\t\t\t\t\t\t\t\t])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVnFkK3mRhZG",
        "colab_type": "text"
      },
      "source": [
        "**Import dataset CIFAR-100**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_acReX5Rhkh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "9cd0a2aeaa394804914c4ec550fd7ae1",
            "76b76c44013c4858b5825f046dc86b58",
            "4d611219a5764f45ac99db6a5c11b810",
            "123156ec1cf14659a5de3e78a55987a5",
            "aa2ef208a83743c49a5ce0a628909248",
            "f83a5c3a30f54e75b9e765c0c19784f5",
            "33a2fd94a7694881917f98d01cf392ea",
            "e71eef3d073a40608a4684b340ce59ef"
          ]
        },
        "outputId": "5a783243-fa8f-46f5-84f7-356eb9b94e20"
      },
      "source": [
        "#For any information about CIFAR-100 follow the link below\n",
        "#https://www.cs.toronto.edu/~kriz/cifar.html\n",
        "\n",
        "train_dataset = CIFAR100(DATA_DIR, train=True, download=True, transform=train_transform)\n",
        "test_dataset = CIFAR100(DATA_DIR, train=False, download=False, transform=test_transform)\n",
        "\n",
        "if SHUFFLE_CLASSES:\n",
        "  # --- Shuffle class ordering\n",
        "  if RANDOM_STATE == 'telegram':\n",
        "    classes_indexes = np.array([30,  4, 36, 47, 81, 65, 66, 64, 68, 23, 72, 48, 54, 73,  6, 50, 51,\n",
        "                          83, 75, 88, 58, 62, 39, 60, 94, 25, 84, 37, 33, 76, 34, 57, 46,  3,\n",
        "                          24, 67, 17, 79, 40, 77, 26, 27, 41, 90, 89, 59, 20, 11, 61, 13, 44,\n",
        "                          56,  9, 96, 70, 99, 82, 78,  5, 53, 16, 29,  0, 31,  7, 74, 55, 19,\n",
        "                          42,  1, 92, 63, 52, 69, 22, 18, 28, 35,  8, 91, 86, 32, 97, 98, 15,\n",
        "                            2, 45, 49, 95, 71, 14, 87, 80, 21, 38, 93, 43, 10, 12, 85])\n",
        "    \n",
        "  elif RANDOM_STATE == 'tarantino':\n",
        "    random.seed(653)\n",
        "    classes_indexes = [i for i in range(NUM_CLASSES)]\n",
        "\n",
        "    classes_indexes_cum = []\n",
        "    remaining = [i for i in range(NUM_CLASSES)]\n",
        "    for i in range(10):\n",
        "      classes_indexes_cum += random.sample(remaining, 10)\n",
        "      remaining = list(set(classes_indexes)-set(classes_indexes_cum))\n",
        "\n",
        "    classes_indexes = classes_indexes_cum\n",
        "    classes_indexes = np.array(classes_indexes)\n",
        "\n",
        "    print('Tarantino classes order:', classes_indexes)\n",
        "\n",
        "  else:\n",
        "    if RANDOM_STATE is not None:\n",
        "      np.random.seed(RANDOM_STATE)\n",
        "\n",
        "    classes_indexes = np.array([i for i in range(NUM_CLASSES)])\n",
        "    np.random.shuffle(classes_indexes)\n",
        "\n",
        "\n",
        "  classes_shuffle_dict = {ind:i for i, ind in enumerate(classes_indexes)}\n",
        "\n",
        "  train_dataset.targets = [classes_shuffle_dict[tar] for tar in train_dataset.targets]\n",
        "  test_dataset.targets = [classes_shuffle_dict[tar] for tar in test_dataset.targets]\n",
        "\n",
        "  CLASSES = train_dataset.classes\n",
        "  train_dataset.class_to_idx = {CLASSES[i]:ind for i,ind in enumerate(classes_indexes)}\n",
        "  LABEL_INDEX_DICT = train_dataset.class_to_idx\n",
        "else:\n",
        "  CLASSES = train_dataset.classes\n",
        "  LABEL_INDEX_DICT = train_dataset.class_to_idx\n",
        "\n",
        "# show_random_images(train_dataset, 5, mean=MEANS, std=STDS)\n",
        "\n",
        "print('Train Dataset length:', len(train_dataset))\n",
        "print('Test Dataset length:', len(test_dataset))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./CIFAR_100/cifar-100-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9cd0a2aeaa394804914c4ec550fd7ae1",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./CIFAR_100/cifar-100-python.tar.gz to ./CIFAR_100\n",
            "Train Dataset length: 50000\n",
            "Test Dataset length: 10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4ZhjI56LOIt",
        "colab_type": "text"
      },
      "source": [
        "**Prepare training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f70xiaS4LNo6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = resnet32(num_classes=NUM_CLASSES)\n",
        "icarl = iCaRL(device=DEVICE, batch_size=BATCH_SIZE, K=K, dataset=train_dataset)\n",
        "\n",
        "# Define loss function\n",
        "criterion = nn.BCEWithLogitsLoss(reduction='mean') # reduction='sum' is crucial as BCE is designed for one output neuron only (it averages on batch_size*num_classes instead of on just batch_size) - actually this is why iCaRL keeps a really high learning rate\n",
        "criterion_eval = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFXEmHyHo-Zt",
        "colab_type": "text"
      },
      "source": [
        "**Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaHEiGnro_85",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9cac45dc-a800-4d9c-9888-e71ae3c32e09"
      },
      "source": [
        "cudnn.benchmark # Calling this optimizes runtime\n",
        "\n",
        "val_indexes_cum = []\n",
        "test_indexes_cum = []\n",
        "current_classes_cum = []\n",
        "\n",
        "group_losses_train = []\n",
        "group_losses_eval = []\n",
        "group_accuracies_train = []\n",
        "group_accuracies_eval = []\n",
        "group_accuracies_eval_curr = []\n",
        "group_accuracies_eval_nme = []\n",
        "\n",
        "now = datetime.datetime.now(timezone('Europe/Rome'))\n",
        "CURRENT_RUN = 'RUN_' + now.strftime(\"%Y-%m-%d %H %M %S\")\n",
        "try:\n",
        "  os.makedirs(RUNS_DIR+'/'+CURRENT_RUN)\n",
        "except OSError:\n",
        "  print (\"FATAL ERROR - Creation of the directory of the current run failed\")\n",
        "  sys.exit()\n",
        "\n",
        "dump_hyperparameters(path=RUNS_DIR+'/'+CURRENT_RUN, lr=LR, weight_decay=WEIGHT_DECAY, num_epochs=NUM_EPOCHS, method=METHOD, batch_size=BATCH_SIZE)\n",
        "\n",
        "START_TIME = time.time()\n",
        "\n",
        "for group_number in range(N_GROUPS_FOR_TRAINING):\n",
        "\n",
        "  starting_label = (group_number*10)\n",
        "  ending_label = (group_number+1)*10\n",
        "  current_classes = list(range(starting_label, ending_label))\n",
        "\n",
        "  new_indexes = get_indexes_from_labels(train_dataset, current_classes)\n",
        "\n",
        "  # np.random.shuffle(new_indexes)\n",
        "\n",
        "  train_dataset_curr = Subset(train_dataset, new_indexes)\n",
        "  exemplars = icarl.flattened_exemplars()\n",
        "  train_dataset_cum_exemplars = Subset(train_dataset, exemplars+new_indexes)\n",
        "\n",
        "  # Update training set\n",
        "  train_dataloader = DataLoader(train_dataset_curr, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
        "  train_dataloader_cum_exemplars = DataLoader(train_dataset_cum_exemplars, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
        "\n",
        "  train_dataloader_for_evaluation = DataLoader(train_dataset_curr, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "  train_dataloader_cum_exemplars_for_evaluation = DataLoader(train_dataset_cum_exemplars, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "\n",
        "  # Update test set\n",
        "  new_test_indexes = get_indexes_from_labels(test_dataset, current_classes)\n",
        "  test_dataset_cum = Subset(test_dataset, test_indexes_cum+new_test_indexes)\n",
        "  test_dataset_curr = Subset(test_dataset, new_test_indexes)\n",
        "\n",
        "  test_indexes_cum += new_test_indexes\n",
        "\n",
        "  test_dataloader = DataLoader(test_dataset_cum, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "  test_dataloader_curr = DataLoader(test_dataset_curr, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "\n",
        "  print('******************************')\n",
        "  print(f'NEW GROUP OF CLASSES {(group_number+1)}Â°/{N_GROUPS_FOR_TRAINING}')\n",
        "  print('Training set length:', len(train_dataset_curr))\n",
        "  if USE_VALIDATION_SET:\n",
        "    print('Validation set length:', len(val_dataset_cum))\n",
        "  print('Test set length:', len(test_dataset_cum))\n",
        "  \n",
        "  net = net.to(DEVICE)\n",
        "\n",
        "  parameters_to_optimize = net.parameters()\n",
        "\n",
        "  optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "  milestone_1 = math.floor(NUM_EPOCHS/10*7)\n",
        "  milestone_2 = math.floor(NUM_EPOCHS/10*9)\n",
        "\n",
        "  scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[milestone_1, milestone_2], gamma=GAMMA)\n",
        "\n",
        "  current_step = 0\n",
        "  losses_train = []\n",
        "  losses_eval = []\n",
        "  accuracies_train = []\n",
        "  accuracies_eval = []\n",
        "  accuracies_eval_curr = []\n",
        "  accuracies_eval_nme = []\n",
        "\n",
        "  net_old = None\n",
        "  if starting_label > 0:\n",
        "    # Salva la rete attuale per calcolare i vecchi outputs\n",
        "    net_old = deepcopy(net)\n",
        "\n",
        "  net.train()\n",
        "  for epoch in range(NUM_EPOCHS):\n",
        "    print('Starting epoch {}/{}, LR = {}'.format(epoch+1, NUM_EPOCHS, scheduler.get_lr()))\n",
        "\n",
        "    #\n",
        "    # Update weights using iCaRL BCE and distillation loss on Dataset\n",
        "    #\n",
        "    loss = icarl.update_representation(net, net_old, train_dataloader_cum_exemplars, criterion, optimizer, current_classes, starting_label, ending_label, current_step, bce_var=BCE_VAR, loss_type=LOSS_TYPE, alpha=ALPHA)\n",
        "\n",
        "    current_step += 1\n",
        "    scheduler.step()\n",
        "\n",
        "    print('--- Epoch {}, Loss on train: {}'.format(epoch+1, loss.item()))\n",
        "    losses_train.append(loss.item())\n",
        "\n",
        "\n",
        "  # --- END OF TRAINING FOR THIS GROUP OF CLASSES\n",
        "  print('Length on train dataset (exemplars included):', len(train_dataset_cum_exemplars))\n",
        "\n",
        "  #\n",
        "  # Compute means of each class using the entire current training set and the exemplars\n",
        "  #\n",
        "  icarl.compute_means(net, train_dataloader_cum_exemplars, ending_label)\n",
        "\n",
        "  if starting_label > 0:\n",
        "    #\n",
        "    # Reduce number of exemplars for each class to 2000/ending_label\n",
        "    #\n",
        "    icarl.reduce_exemplars(starting_label, ending_label)\n",
        " \n",
        "  #\n",
        "  # Construct exemplars for future evaluation\n",
        "  #\n",
        "  icarl.construct_exemplars(net, starting_label, ending_label, herding=USE_HERDING)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    _, accuracy_test = eval_model(net, test_dataloader, criterion=criterion_eval,\n",
        "                                          dataset_length=len(test_dataset_cum), use_bce_loss=None,\n",
        "                                          ending_label=ending_label, loss=False, device=DEVICE, display=True, suffix=' (group)')\n",
        "  losses_eval.append(-1)\n",
        "  accuracies_eval.append(accuracy_test)\n",
        "\n",
        "  #\n",
        "  # Eval model using NME on test set\n",
        "  #\n",
        "  with torch.no_grad():\n",
        "    accuracy_eval_nme = icarl.eval_model_nme(net, test_dataloader, dataset_length=len(test_dataset_cum), display=True, suffix=' (group)')\n",
        "  accuracies_eval_nme.append(accuracy_eval_nme)\n",
        "\n",
        "  #\n",
        "  # Accuracy on training\n",
        "  #\n",
        "  with torch.no_grad():\n",
        "    accuracy_train = eval_model_accuracy(net, train_dataloader_for_evaluation, dataset_length=len(train_dataset_curr), starting_label=starting_label, ending_label=ending_label, device=DEVICE, display=True, suffix='train (group)')\n",
        "  accuracies_train.append(accuracy_train)\n",
        "\n",
        "  #\n",
        "  # Compute accuracy on test for novel classes only\n",
        "  #\n",
        "  with torch.no_grad():\n",
        "    accuracy_eval_curr_classes = eval_model_accuracy(net, test_dataloader_curr, dataset_length=len(test_dataset_curr), starting_label=starting_label, ending_label=ending_label, device=DEVICE, display=True, suffix='test novel classes (group)')\n",
        "  accuracies_eval_curr.append(accuracy_eval_curr_classes)\n",
        "\n",
        "  path = RUNS_DIR+'/'+CURRENT_RUN    \n",
        "  create_dir_for_current_group(group_number, path=path)\n",
        "  \n",
        "  draw_graphs(losses_train,\n",
        "        losses_eval,\n",
        "        accuracies_train,\n",
        "        accuracies_eval,\n",
        "        num_epochs=NUM_EPOCHS, use_validation=USE_VALIDATION_SET, print_img=False, save=True, path=path, group_number=group_number)\n",
        "  \n",
        "  dump_to_csv(losses_train,\n",
        "        losses_eval,\n",
        "        accuracies_train,\n",
        "        accuracies_eval,\n",
        "        group_number=group_number, path=path)\n",
        "\n",
        "  group_losses_train.append(losses_train[-1])\n",
        "  group_losses_eval.append(losses_eval[-1])\n",
        "  group_accuracies_train.append(accuracies_train[-1])\n",
        "  group_accuracies_eval.append(accuracies_eval[-1])\n",
        "  group_accuracies_eval_curr.append(accuracies_eval_curr[-1])\n",
        "  group_accuracies_eval_nme.append(accuracies_eval_nme[-1])\n",
        "\n",
        "# END OF OVERALL TRAINING\n",
        "dump_final_values_nme(group_losses_train, group_accuracies_train, group_accuracies_eval_nme, group_accuracies_eval, group_accuracies_eval_curr, path=RUNS_DIR+'/'+CURRENT_RUN)\n",
        "draw_final_graphs_nme(group_losses_train, group_accuracies_eval_nme, group_accuracies_eval, use_validation=USE_VALIDATION_SET, print_img=True, save=True, path=RUNS_DIR+'/'+CURRENT_RUN)\n",
        "\n",
        "print('Average incremental accuracy (nme)', np.mean(group_accuracies_eval_nme))\n",
        "print('Average incremental accuracy (hybrid 1)', np.mean(group_accuracies_eval))\n",
        "\n",
        "#\n",
        "# Compute and display confusion matrix\n",
        "#\n",
        "conf_mat = get_conf_matrix_nme(net, test_dataloader, icarl=icarl, ending_label=ending_label, device=DEVICE)\n",
        "display_conf_matrix(conf_mat, display=True, save=True, path=RUNS_DIR+'/'+CURRENT_RUN)\n",
        "\n",
        "DURATION = round((time.time()-START_TIME)/60, 1)\n",
        "print(f\"> In {(DURATION)} minutes\")\n",
        "\n",
        "github_link = 'https://github.com/gabrieletiboni/Incremental-learning-on-image-recognition/tree/master/RUNS/'+str(CURRENT_RUN)\n",
        "github_link = github_link.replace(\" \", \"%20\")\n",
        "hyperparameters_string = get_hyperparameter_string(lr=LR, weight_decay=WEIGHT_DECAY, num_epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, multilrstep=DO_MULTILR_STEP_DOWN, gamma=GAMMA)\n",
        "extra_hyper_str = 'LOSS_TYPE='+str(LOSS_TYPE)+', ALPHA='+str(ALPHA)\n",
        "if DUMP_FINAL_RESULTS_ON_GSPREADSHEET:\n",
        "  dump_on_gspreadsheet_nme(CURRENT_RUN, GITHUB_USER, github_link, METHOD, RANDOM_STATE, USE_HERDING,\n",
        "                           CIFAR_NORMALIZE, BCE_VAR, group_losses_train, group_accuracies_train, group_accuracies_eval_nme,\n",
        "                           group_accuracies_eval, group_accuracies_eval_curr, DURATION,\n",
        "                           hyperparameters=hyperparameters_string, ablation='loss', params=extra_hyper_str)\n",
        "\n",
        "beep()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "******************************\n",
            "NEW GROUP OF CLASSES 1Â°/10\n",
            "Training set length: 5000\n",
            "Test set length: 1000\n",
            "Starting epoch 1/70, LR = [0.01]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:396: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "--- Initial loss on train: 5.092689037322998\n",
            "--- Epoch 1, Loss on train: 1.9101066589355469\n",
            "Starting epoch 2/70, LR = [0.01]\n",
            "--- Epoch 2, Loss on train: 1.8924338817596436\n",
            "Starting epoch 3/70, LR = [0.01]\n",
            "--- Epoch 3, Loss on train: 1.5586024522781372\n",
            "Starting epoch 4/70, LR = [0.01]\n",
            "--- Epoch 4, Loss on train: 1.2977646589279175\n",
            "Starting epoch 5/70, LR = [0.01]\n",
            "--- Epoch 5, Loss on train: 1.3609105348587036\n",
            "Starting epoch 6/70, LR = [0.01]\n",
            "--- Epoch 6, Loss on train: 1.3253732919692993\n",
            "Starting epoch 7/70, LR = [0.01]\n",
            "--- Epoch 7, Loss on train: 1.029186487197876\n",
            "Starting epoch 8/70, LR = [0.01]\n",
            "--- Epoch 8, Loss on train: 1.1102126836776733\n",
            "Starting epoch 9/70, LR = [0.01]\n",
            "--- Epoch 9, Loss on train: 1.038163423538208\n",
            "Starting epoch 10/70, LR = [0.01]\n",
            "--- Epoch 10, Loss on train: 0.993091344833374\n",
            "Starting epoch 11/70, LR = [0.01]\n",
            "--- Epoch 11, Loss on train: 1.207373023033142\n",
            "Starting epoch 12/70, LR = [0.01]\n",
            "--- Epoch 12, Loss on train: 0.9041563272476196\n",
            "Starting epoch 13/70, LR = [0.01]\n",
            "--- Epoch 13, Loss on train: 0.9097588658332825\n",
            "Starting epoch 14/70, LR = [0.01]\n",
            "--- Epoch 14, Loss on train: 0.9631344079971313\n",
            "Starting epoch 15/70, LR = [0.01]\n",
            "--- Epoch 15, Loss on train: 0.8583583235740662\n",
            "Starting epoch 16/70, LR = [0.01]\n",
            "--- Epoch 16, Loss on train: 0.660832941532135\n",
            "Starting epoch 17/70, LR = [0.01]\n",
            "--- Epoch 17, Loss on train: 0.9167798757553101\n",
            "Starting epoch 18/70, LR = [0.01]\n",
            "--- Epoch 18, Loss on train: 0.82817542552948\n",
            "Starting epoch 19/70, LR = [0.01]\n",
            "--- Epoch 19, Loss on train: 0.6293032765388489\n",
            "Starting epoch 20/70, LR = [0.01]\n",
            "--- Epoch 20, Loss on train: 0.6234974265098572\n",
            "Starting epoch 21/70, LR = [0.01]\n",
            "--- Epoch 21, Loss on train: 0.7739970684051514\n",
            "Starting epoch 22/70, LR = [0.01]\n",
            "--- Epoch 22, Loss on train: 0.6187644600868225\n",
            "Starting epoch 23/70, LR = [0.01]\n",
            "--- Epoch 23, Loss on train: 0.5856647491455078\n",
            "Starting epoch 24/70, LR = [0.01]\n",
            "--- Epoch 24, Loss on train: 0.5993020534515381\n",
            "Starting epoch 25/70, LR = [0.01]\n",
            "--- Epoch 25, Loss on train: 0.6277437210083008\n",
            "Starting epoch 26/70, LR = [0.01]\n",
            "--- Epoch 26, Loss on train: 0.5659682154655457\n",
            "Starting epoch 27/70, LR = [0.01]\n",
            "--- Epoch 27, Loss on train: 0.37093889713287354\n",
            "Starting epoch 28/70, LR = [0.01]\n",
            "--- Epoch 28, Loss on train: 0.4840163290500641\n",
            "Starting epoch 29/70, LR = [0.01]\n",
            "--- Epoch 29, Loss on train: 0.5241426825523376\n",
            "Starting epoch 30/70, LR = [0.01]\n",
            "--- Epoch 30, Loss on train: 0.631453275680542\n",
            "Starting epoch 31/70, LR = [0.01]\n",
            "--- Epoch 31, Loss on train: 0.4049316346645355\n",
            "Starting epoch 32/70, LR = [0.01]\n",
            "--- Epoch 32, Loss on train: 0.5392666459083557\n",
            "Starting epoch 33/70, LR = [0.01]\n",
            "--- Epoch 33, Loss on train: 0.3665871322154999\n",
            "Starting epoch 34/70, LR = [0.01]\n",
            "--- Epoch 34, Loss on train: 0.6676981449127197\n",
            "Starting epoch 35/70, LR = [0.01]\n",
            "--- Epoch 35, Loss on train: 0.5428680181503296\n",
            "Starting epoch 36/70, LR = [0.01]\n",
            "--- Epoch 36, Loss on train: 0.3438454866409302\n",
            "Starting epoch 37/70, LR = [0.01]\n",
            "--- Epoch 37, Loss on train: 0.33172231912612915\n",
            "Starting epoch 38/70, LR = [0.01]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}